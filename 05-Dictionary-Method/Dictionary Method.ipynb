{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary Method\n",
    "\n",
    "For the final two days we'll move to measuring the prevelence of themes in a corpus. We'll cover three ways of doing this: the dictionary method, supervised classification, and unsupervised machine learning. Today, dictionary method.\n",
    "\n",
    "This is the most simple way to measure the prevelence of a theme in a corpus, and is used for many purposes, including sentiment analysis. This is one of the most long-standing, and ubiquitous, methods in automated text analysis, so it's important to both understand the method and be able to implement it.\n",
    "\n",
    "The method is simple: it involves grouping words into categories or themes, and then counting the number of words from each theme in your corpus. We will use this method to do sentiment analysis, a popular text analysis task, on our Music Review corpus, using a standard sentiment analysis dictionary.\n",
    "\n",
    "### Learning Goals\n",
    "* Understand the intuition behind the dictionary method\n",
    "* Learn how to implement in via Python Pandas and NLTK\n",
    "* Get more comfortable combining Python packages together for more powerful analytic power\n",
    "    * Today, we'll combine Pandas and NLTK\n",
    "* Implement a rudimentary sentiment analysis tool\n",
    "\n",
    "\n",
    "### Outline\n",
    "* Introduction to the Dictionary Method\n",
    "* Pre-Processing\n",
    "    * Creat Pandas DF\n",
    "    * Lowercase, remove punctuation, tokenize\n",
    "    * Create column for token count\n",
    "* Sentiment Analysis using the Dictionary Method\n",
    "\n",
    "\n",
    "### Key Jargon\n",
    "\n",
    "* *dictionary method*:\n",
    "    * text analysis method that utilizes the frequency of key words, grouped into themes, to determine the prevelance of that theme throughout a corpus.\n",
    "* *standard dictionary*:\n",
    "    * otherwise known as general dictionaries, a dictionary created by experts meant to measure general phenomenon.\n",
    "* *custom dictionary*:\n",
    "    * dictionaries tailored to a specific domain or question. Usually created by the researcher based on the research question.\n",
    "* *sentiment analysis*:\n",
    "    * the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc., is positive, negative, or neutral.\n",
    "* *lambda function*:\n",
    "    * A function that your write yourself. This is different than the built-in functions we have been using.\n",
    "\n",
    "### Further Resources\n",
    "\n",
    "[A Novel Method for Detecting Plot](http://www.matthewjockers.net/2014/06/05/a-novel-method-for-detecting-plot/), Matt Jockers\n",
    "\n",
    "Enns, Peter, Nathan Kelly, Jana Morgan, and Christopher Witko. 2015.[“Money and the Supply\n",
    "of Political Rhetoric: Understanding the Congressional (Non-)Response to Economic Inequality.”](http://cdn.equitablegrowth.org/wp-content/uploads/2016/06/29155322/enns-kelly-morgan-witko-econinterests-policyagenda.pdf) Paper presented at the APSA Annual Meetings, San Francisco.\n",
    "* Outlines the process of creating your own dictionary\n",
    "\n",
    "[Neal Caren has a tutorial using MPQA](http://nealcaren.web.unc.edu/an-introduction-to-text-analysis-with-python-part-3/), which implements the dictionary method in Python but in a much different way \n",
    "\n",
    "**__________________________________**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Introduction to the Dictionary Method\n",
    "\n",
    "The dictionary method is based on the assumption that themes or categories consist of a group of words, and texts that cover that theme will have a higher percentage of that group of words compared to other texts. Dictionary methods are used for many purposes. A few possibilities:\n",
    "* classify text into themes\n",
    "* measure the *tone* of text\n",
    "* measure sentiment\n",
    "* measure psychological processes\n",
    "\n",
    "There are two forms of dictionaries: standard or general dictionaries, and custom dictionaries.\n",
    "\n",
    "#### Standard Dictionaries\n",
    "\n",
    "There are a number of standard dictionaries that have been created by field experts. The benefit of standarized dictionaries is that they're developed by experts and have been throughoughly validated. Others have likely published using these dictionaries, so reviewers are more likely to accept them as valid. Because of this, they are good options if they fit your research question. \n",
    "\n",
    "Here are a few:\n",
    "\n",
    "* [DICTION](http://www.dictionsoftware.com/): a computer-aided text analysis program for determining the tone of a text. It was created by and for organization scholars and political scientists.\n",
    "    * Main five categories: Certainty, Activity, Optimism, Realism, Commonality\n",
    "    * 35 sub-categories\n",
    "    * Allows you to create your own dictionary\n",
    "    * Proprietary software\n",
    "* [Linguistic Inquiry and Word Count (LIWC)](http://liwc.wpengine.com/): Created by psychologists, it's meant to capture psychological processes around feelings, personality, and motivations. It's also proprietary.\n",
    "* [Multi-Perspective Question Answering (MPQA)](http://mpqa.cs.pitt.edu/): The free version of LIWC. We will use this dictionary today.\n",
    "* [Harvard General Inquirer](http://www.wjh.harvard.edu/~inquirer/spreadsheet_guide.htm). Multiple categories, including abstract and concrete words. It's free and available online.\n",
    "\n",
    "#### Custom Dictionaries\n",
    "\n",
    "Many research questions or data are domain specific, however, and will thus require you to create your own dictionary based on your own knowledge of the domain and question. Creating your own dictionary requires a lot of thought, and must be validated. These dictionaries are typically created in an interative fashion, and are modified as they are validated. See Enns et al. (2015) for an example of how they constructed their own dictionary. \n",
    "\n",
    "Today we will use the free and standard sentiment dictionary from MPQA to measure positive and negative sentiment in the music reviews.\n",
    "\n",
    "Our first step, as with any technique, is the pre-processing step, to get the data ready for analyis.\n",
    "\n",
    "### 1. Pre-Processing\n",
    "\n",
    "First, read in our Music Reviews corpus as a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    album  \\\n",
      "0                             Don't Panic   \n",
      "1                 Fear and Saturday Night   \n",
      "2                      The Way I'm Livin'   \n",
      "3                                   Doris   \n",
      "4                                 Giraffe   \n",
      "5                            Weathervanes   \n",
      "6                    Build a Rocket Boys!   \n",
      "7                      Ambivalence Avenue   \n",
      "8                                 Wavvves   \n",
      "9                          Peachtree Road   \n",
      "10                               Heritage   \n",
      "11                            White Chalk   \n",
      "12                    Tyrannosaurus Hives   \n",
      "13                             JackInABox   \n",
      "14                            Liquid Love   \n",
      "15                  The  Truth About Love   \n",
      "16                            The Monitor   \n",
      "17                         Ones and Sixes   \n",
      "18        In Search Of... [First Version]   \n",
      "19                            Tarot Sport   \n",
      "20                             July Flame   \n",
      "21                                    Lux   \n",
      "22                    Live At The Olympia   \n",
      "23                     Ten Thousand Fists   \n",
      "24                             New Danger   \n",
      "25                                    NYC   \n",
      "26                 Hold On Now, Youngster   \n",
      "27                       Tiny Rebels [EP]   \n",
      "28                       Wavering Radiant   \n",
      "29                          Sound Mirrors   \n",
      "...                                   ...   \n",
      "4971           This Machine Kills Artists   \n",
      "4972                       The Fire Theft   \n",
      "4973           XI Versions of Black Noise   \n",
      "4974              I Dreamed We Fell Apart   \n",
      "4975                         Beat Pyramid   \n",
      "4976                       Freedom's Road   \n",
      "4977          Scott Pilgrim Vs. The World   \n",
      "4978                Little Neon Limelight   \n",
      "4979                                 Hawk   \n",
      "4980                       Single Mothers   \n",
      "4981                        Lost Horizons   \n",
      "4982               Dry Land is Not a Myth   \n",
      "4983                          Pain Killer   \n",
      "4984                    Swing Lo Magellan   \n",
      "4985                             Maraqopa   \n",
      "4986                                  Son   \n",
      "4987                              Holiday   \n",
      "4988                              Majenta   \n",
      "4989                         Love Is Here   \n",
      "4990                                 Days   \n",
      "4991                              Rebirth   \n",
      "4992                           Blue Songs   \n",
      "4993                England, Half English   \n",
      "4994                   Weezer (Red Album)   \n",
      "4995                Dreams and Nightmares   \n",
      "4996                          Outer South   \n",
      "4997                         On An Island   \n",
      "4998                             Movement   \n",
      "4999                          Locked Down   \n",
      "5000  And Their Refinement Of The Decline   \n",
      "\n",
      "                                       artist       genre  \\\n",
      "0                                All Time Low    Pop/Rock   \n",
      "1                                Ryan Bingham     Country   \n",
      "2                              Lee Ann Womack     Country   \n",
      "3                             Earl Sweatshirt         Rap   \n",
      "4                                     Echoboy        Rock   \n",
      "5                            Freelance Whales       Indie   \n",
      "6                                       Elbow    Pop/Rock   \n",
      "7                                       Bibio       Indie   \n",
      "8                                      Wavves       Indie   \n",
      "9                                  Elton John        Rock   \n",
      "10                                    College  Electronic   \n",
      "11                                  PJ Harvey        Rock   \n",
      "12                                  The Hives        Rock   \n",
      "13                               Turin Brakes       Indie   \n",
      "14                                  Shy Child       Indie   \n",
      "15                                       P!nk         Pop   \n",
      "16                           Titus Andronicus       Indie   \n",
      "17                                        Low    Pop/Rock   \n",
      "18                    N.E.R.D. [The Neptunes]         Rap   \n",
      "19                               Fuck Buttons        Rock   \n",
      "20                                Laura Veirs       Indie   \n",
      "21                                  Brian Eno  Electronic   \n",
      "22                                     R.E.M.        Rock   \n",
      "23                                  Disturbed        Rock   \n",
      "24                                    Mos Def         Rap   \n",
      "25               Kieran Hebden and Steve Reid       Indie   \n",
      "26                            Los Campesinos!       Indie   \n",
      "27                                 Cairo Gang    Pop/Rock   \n",
      "28                                       Isis        Rock   \n",
      "29                                    Coldcut  Electronic   \n",
      "...                                       ...         ...   \n",
      "4971                               King Buzzo    Pop/Rock   \n",
      "4972                           The Fire Theft       Indie   \n",
      "4973                         Pantha du Prince  Electronic   \n",
      "4974                                  Memphis       Indie   \n",
      "4975                       These New Puritans        Rock   \n",
      "4976                          John Mellencamp        Rock   \n",
      "4977                      Original Soundtrack        Rock   \n",
      "4978                               Houndmouth     Country   \n",
      "4979                          Isobel Campbell    Pop/Rock   \n",
      "4980                      Justin Townes Earle     Country   \n",
      "4981                              Lemon Jelly  Electronic   \n",
      "4982                             White Arrows    Pop/Rock   \n",
      "4983                          Little Big Town     Country   \n",
      "4984                         Dirty Projectors    Pop/Rock   \n",
      "4985                            Damien Jurado    Pop/Rock   \n",
      "4986                             Juana Molina       Indie   \n",
      "4987                          Port St. Willow    Pop/Rock   \n",
      "4988                              Jimmy Edgar  Electronic   \n",
      "4989                               Starsailor        Rock   \n",
      "4990                              Real Estate    Pop/Rock   \n",
      "4991                                Lil Wayne         Rap   \n",
      "4992                   Hercules & Love Affair  Electronic   \n",
      "4993                              Billy Bragg        Rock   \n",
      "4994                                   Weezer        Rock   \n",
      "4995                                Meek Mill         Rap   \n",
      "4996  Conor Oberst And The Mystic Valley Band       Indie   \n",
      "4997                            David Gilmour        Rock   \n",
      "4998                                   Gossip       Indie   \n",
      "4999                                 Dr. John    Pop/Rock   \n",
      "5000                         Stars Of The Lid        Rock   \n",
      "\n",
      "             release_date                                      critic  score  \\\n",
      "0     2012-10-09 00:00:00                                    Kerrang!   74.0   \n",
      "1     2015-01-20 00:00:00                                       Uncut   70.0   \n",
      "2     2014-09-23 00:00:00                                  Q Magazine   84.0   \n",
      "3     2013-08-20 00:00:00                                   Pitchfork   82.0   \n",
      "4     2003-02-25 00:00:00                                    AllMusic   71.0   \n",
      "5     2010-04-13 00:00:00                                  Q Magazine   68.0   \n",
      "6     2011-04-12 00:00:00                       Delusions of Adequacy   82.0   \n",
      "7     2009-06-23 00:00:00                                  Q Magazine   78.0   \n",
      "8     2009-03-17 00:00:00                                  PopMatters   68.0   \n",
      "9     2004-11-09 00:00:00                                       MelD.   70.0   \n",
      "10    2013-09-17 00:00:00                                musicOMH.com   63.0   \n",
      "11    2007-09-25 00:00:00                              Paste Magazine   80.0   \n",
      "12    2004-07-20 00:00:00                                  Playlouder   78.0   \n",
      "13    2005-06-07 00:00:00                   New Musical Express (NME)   62.0   \n",
      "14    2010-03-15 00:00:00                                The Guardian   70.0   \n",
      "15    2012-09-18 00:00:00                                The Guardian   77.0   \n",
      "16    2010-03-09 00:00:00                             Prefix Magazine   82.0   \n",
      "17    2015-09-11 00:00:00                        Consequence of Sound   78.0   \n",
      "18    2001-08-06 00:00:00                                  Q Magazine   92.0   \n",
      "19    2009-10-20 00:00:00                               The A.V. Club   84.0   \n",
      "20    2010-01-12 00:00:00                                musicOMH.com   81.0   \n",
      "21    2012-11-13 00:00:00                              Paste Magazine   75.0   \n",
      "22    2009-10-27 00:00:00                               The A.V. Club   74.0   \n",
      "23    2005-09-20 00:00:00                                  Amazon.com   59.0   \n",
      "24    2004-10-12 00:00:00                            Austin Chronicle   59.0   \n",
      "25    2008-11-18 00:00:00                                    The Wire   61.0   \n",
      "26    2008-04-01 00:00:00                            Austin Chronicle   81.0   \n",
      "27    2013-07-23 00:00:00                                       Uncut   64.0   \n",
      "28    2009-05-05 00:00:00                                  No Ripcord   79.0   \n",
      "29    2006-02-21 00:00:00                                  Playlouder   73.0   \n",
      "...                   ...                                         ...    ...   \n",
      "4971  2014-06-03 00:00:00                        The Line of Best Fit   69.0   \n",
      "4972  2003-09-23 00:00:00                           Alternative Press   63.0   \n",
      "4973  2011-04-19 00:00:00                          The Boston Phoenix   62.0   \n",
      "4974  2004-09-21 00:00:00                                  Launch.com   67.0   \n",
      "4975  2008-03-18 00:00:00                                  Q Magazine   76.0   \n",
      "4976  2007-01-23 00:00:00                                     Blender   65.0   \n",
      "4977  2010-08-10 00:00:00                             ImmersedInSound   68.0   \n",
      "4978  2015-03-17 00:00:00                                    AllMusic   74.0   \n",
      "4979  2010-08-24 00:00:00                                   Pitchfork   75.0   \n",
      "4980  2014-09-09 00:00:00                               The A.V. Club   73.0   \n",
      "4981  2002-10-08 00:00:00                                       Uncut   81.0   \n",
      "4982  2012-06-19 00:00:00                                    AllMusic   70.0   \n",
      "4983  2014-10-21 00:00:00                                   joyel1992   83.0   \n",
      "4984  2012-07-10 00:00:00                                  PopMatters   80.0   \n",
      "4985  2012-02-20 00:00:00  Beats Per Minute (formerly One Thirty BPM)   81.0   \n",
      "4986  2006-06-06 00:00:00                                    The Wire   79.0   \n",
      "4987  2013-04-02 00:00:00                                musicOMH.com   80.0   \n",
      "4988  2012-12-04 00:00:00                                   Pitchfork   59.0   \n",
      "4989  2002-01-08 00:00:00                                  Screenager   72.0   \n",
      "4990  2011-10-18 00:00:00                              Blurt Magazine   77.0   \n",
      "4991  2010-02-02 00:00:00                                  No Ripcord   37.0   \n",
      "4992  2011-08-16 00:00:00                                  PopMatters   68.0   \n",
      "4993  2002-03-05 00:00:00                                        Spin   64.0   \n",
      "4994  2008-06-03 00:00:00                              Slant Magazine   64.0   \n",
      "4995  2012-10-30 00:00:00                                    AllMusic   69.0   \n",
      "4996  2009-05-05 00:00:00                              Slant Magazine   67.0   \n",
      "4997  2006-03-07 00:00:00                                   E! Online   67.0   \n",
      "4998  2003-05-06 00:00:00                                       Uncut   81.0   \n",
      "4999  2012-04-03 00:00:00                                  PopMatters   86.0   \n",
      "5000  2007-04-07 00:00:00                                  PopMatters   87.0   \n",
      "\n",
      "                                                   body  \n",
      "0     While For Baltimore proves they can still writ...  \n",
      "1     There's nothing fake about the purgatorial nar...  \n",
      "2     All life's disastrous lows are here on a caree...  \n",
      "3     With Doris, Odd Future’s Odysseus is finally b...  \n",
      "4     Though Giraffe is definitely Echoboy's most im...  \n",
      "5     Fans of Owl City and The Postal Service will r...  \n",
      "6     Whereas previous Elbow records set a mood, Bui...  \n",
      "7     His remarkable Warp debut follows a series of ...  \n",
      "8     There’s an energy coursing through this, and r...  \n",
      "9     Classic. Songs filled with soul. Lyrics refres...  \n",
      "10    It’s by no means perfect and it does feel slig...  \n",
      "11    Put in context, White Chalk serves her purpose...  \n",
      "12    Although pretty catchy, this album is a tad to...  \n",
      "13     Talk about a fall from grace. [4 Jun 2005, p.58]  \n",
      "14    It's unusual to find a band equally at home wi...  \n",
      "15    It's just a shame she gave album space to Mari...  \n",
      "16    The fundamental difference between The Monitor...  \n",
      "17    It just needs to be a passionate, cathartic, c...  \n",
      "18    They retained their best ideas for themselves ...  \n",
      "19    For most of the songs amassed here, it still t...  \n",
      "20    Laura Veirs makes an excellent case for hersel...  \n",
      "21    Sure, it's beautiful on its own, but without a...  \n",
      "22    Though it isn’t a concept album, Live At The O...  \n",
      "23    The album isn't without its problems––come the...  \n",
      "24    The New Danger is as overextended as it is sel...  \n",
      "25    Hebden is clearly striving for dancefloor impa...  \n",
      "26    It might be one big, saccharine, catchy fuck-y...  \n",
      "27    The Cairo Gang's superb, weighty, meticulous a...  \n",
      "28    As snobbish as that may sound, you have to los...  \n",
      "29    Any number of tracks here could easily catapul...  \n",
      "...                                                 ...  \n",
      "4971  It is an interesting experiment, but realistic...  \n",
      "4972  Yes, it sounds like Yes, and, no, I don't mean...  \n",
      "4973  The pointlessness is grating. XI Versions' fin...  \n",
      "4974  The basic ingredients are delicate, minimal, w...  \n",
      "4975  Anyone bored by the kitchen sink will find muc...  \n",
      "4976  There are two vastly different Mellencamps. On...  \n",
      "4977  The movie is incredible. Everything about it i...  \n",
      "4978  Houndmouth have the right touch and impressive...  \n",
      "4979  Hawk is very much Campbell's album. She made a...  \n",
      "4980  Earle’s consistency is both his friend and his...  \n",
      "4981  Think early Air meets hip-hop, the West Coast ...  \n",
      "4982  A certain kind of playfulness reigns throughou...  \n",
      "4983  One of the best country album Ive heard in a w...  \n",
      "4984  What really shines through the most on Magella...  \n",
      "4985  These songs are probably Jurado's most ambitio...  \n",
      "4986  What's distinctive about Son is that it is mor...  \n",
      "4987  At times this recording is compelling, entranc...  \n",
      "4988  Unfortunately, more than mediocre tracks or th...  \n",
      "4989  A beautifully crafted debut album and one of t...  \n",
      "4990  A fine batch of bittersweet pop songs that are...  \n",
      "4991             The derivativeness quickly overwhelms.  \n",
      "4992  Blue Songs finds Butler and his crew of collab...  \n",
      "4993  England's exoticism is offset by plenty of tou...  \n",
      "4994  Weezer seems to have driven their old shtick i...  \n",
      "4995  As far as graduations from mixtapes to major-l...  \n",
      "4996  The result is an album that's unfortunately ba...  \n",
      "4997  In the end, Island makes Dave sound like he's ...  \n",
      "4998  Beth Ditto's remarkable gospel holler and ferv...  \n",
      "4999  Dr. John is Dr. John. He's a star, and is on f...  \n",
      "5000  Their work, especially that displayed on Refin...  \n",
      "\n",
      "[5001 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "#import the necessary packages\n",
    "import pandas\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "#read the Music Reviews corpus into a Pandas dataframe\n",
    "df = pandas.read_csv(\"BDHSI2016_music_reviews.csv\", sep = '\\t')\n",
    "\n",
    "#view the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a new column in our dataset that contains tokenized words with all the pre-processing steps.\n",
    "\n",
    "The code here will look slightly different that lesson 1, as we're applying these functions to every row in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   body  \\\n",
      "0     While For Baltimore proves they can still writ...   \n",
      "1     There's nothing fake about the purgatorial nar...   \n",
      "2     All life's disastrous lows are here on a caree...   \n",
      "3     With Doris, Odd Future’s Odysseus is finally b...   \n",
      "4     Though Giraffe is definitely Echoboy's most im...   \n",
      "5     Fans of Owl City and The Postal Service will r...   \n",
      "6     Whereas previous Elbow records set a mood, Bui...   \n",
      "7     His remarkable Warp debut follows a series of ...   \n",
      "8     There’s an energy coursing through this, and r...   \n",
      "9     Classic. Songs filled with soul. Lyrics refres...   \n",
      "10    It’s by no means perfect and it does feel slig...   \n",
      "11    Put in context, White Chalk serves her purpose...   \n",
      "12    Although pretty catchy, this album is a tad to...   \n",
      "13     Talk about a fall from grace. [4 Jun 2005, p.58]   \n",
      "14    It's unusual to find a band equally at home wi...   \n",
      "15    It's just a shame she gave album space to Mari...   \n",
      "16    The fundamental difference between The Monitor...   \n",
      "17    It just needs to be a passionate, cathartic, c...   \n",
      "18    They retained their best ideas for themselves ...   \n",
      "19    For most of the songs amassed here, it still t...   \n",
      "20    Laura Veirs makes an excellent case for hersel...   \n",
      "21    Sure, it's beautiful on its own, but without a...   \n",
      "22    Though it isn’t a concept album, Live At The O...   \n",
      "23    The album isn't without its problems––come the...   \n",
      "24    The New Danger is as overextended as it is sel...   \n",
      "25    Hebden is clearly striving for dancefloor impa...   \n",
      "26    It might be one big, saccharine, catchy fuck-y...   \n",
      "27    The Cairo Gang's superb, weighty, meticulous a...   \n",
      "28    As snobbish as that may sound, you have to los...   \n",
      "29    Any number of tracks here could easily catapul...   \n",
      "...                                                 ...   \n",
      "4971  It is an interesting experiment, but realistic...   \n",
      "4972  Yes, it sounds like Yes, and, no, I don't mean...   \n",
      "4973  The pointlessness is grating. XI Versions' fin...   \n",
      "4974  The basic ingredients are delicate, minimal, w...   \n",
      "4975  Anyone bored by the kitchen sink will find muc...   \n",
      "4976  There are two vastly different Mellencamps. On...   \n",
      "4977  The movie is incredible. Everything about it i...   \n",
      "4978  Houndmouth have the right touch and impressive...   \n",
      "4979  Hawk is very much Campbell's album. She made a...   \n",
      "4980  Earle’s consistency is both his friend and his...   \n",
      "4981  Think early Air meets hip-hop, the West Coast ...   \n",
      "4982  A certain kind of playfulness reigns throughou...   \n",
      "4983  One of the best country album Ive heard in a w...   \n",
      "4984  What really shines through the most on Magella...   \n",
      "4985  These songs are probably Jurado's most ambitio...   \n",
      "4986  What's distinctive about Son is that it is mor...   \n",
      "4987  At times this recording is compelling, entranc...   \n",
      "4988  Unfortunately, more than mediocre tracks or th...   \n",
      "4989  A beautifully crafted debut album and one of t...   \n",
      "4990  A fine batch of bittersweet pop songs that are...   \n",
      "4991             The derivativeness quickly overwhelms.   \n",
      "4992  Blue Songs finds Butler and his crew of collab...   \n",
      "4993  England's exoticism is offset by plenty of tou...   \n",
      "4994  Weezer seems to have driven their old shtick i...   \n",
      "4995  As far as graduations from mixtapes to major-l...   \n",
      "4996  The result is an album that's unfortunately ba...   \n",
      "4997  In the end, Island makes Dave sound like he's ...   \n",
      "4998  Beth Ditto's remarkable gospel holler and ferv...   \n",
      "4999  Dr. John is Dr. John. He's a star, and is on f...   \n",
      "5000  Their work, especially that displayed on Refin...   \n",
      "\n",
      "                                            body_tokens  \n",
      "0     while for baltimore proves they can still writ...  \n",
      "1     there's nothing fake about the purgatorial nar...  \n",
      "2     all life's disastrous lows are here on a caree...  \n",
      "3     with doris, odd future’s odysseus is finally b...  \n",
      "4     though giraffe is definitely echoboy's most im...  \n",
      "5     fans of owl city and the postal service will r...  \n",
      "6     whereas previous elbow records set a mood, bui...  \n",
      "7     his remarkable warp debut follows a series of ...  \n",
      "8     there’s an energy coursing through this, and r...  \n",
      "9     classic. songs filled with soul. lyrics refres...  \n",
      "10    it’s by no means perfect and it does feel slig...  \n",
      "11    put in context, white chalk serves her purpose...  \n",
      "12    although pretty catchy, this album is a tad to...  \n",
      "13     talk about a fall from grace. [4 jun 2005, p.58]  \n",
      "14    it's unusual to find a band equally at home wi...  \n",
      "15    it's just a shame she gave album space to mari...  \n",
      "16    the fundamental difference between the monitor...  \n",
      "17    it just needs to be a passionate, cathartic, c...  \n",
      "18    they retained their best ideas for themselves ...  \n",
      "19    for most of the songs amassed here, it still t...  \n",
      "20    laura veirs makes an excellent case for hersel...  \n",
      "21    sure, it's beautiful on its own, but without a...  \n",
      "22    though it isn’t a concept album, live at the o...  \n",
      "23    the album isn't without its problems––come the...  \n",
      "24    the new danger is as overextended as it is sel...  \n",
      "25    hebden is clearly striving for dancefloor impa...  \n",
      "26    it might be one big, saccharine, catchy fuck-y...  \n",
      "27    the cairo gang's superb, weighty, meticulous a...  \n",
      "28    as snobbish as that may sound, you have to los...  \n",
      "29    any number of tracks here could easily catapul...  \n",
      "...                                                 ...  \n",
      "4971  it is an interesting experiment, but realistic...  \n",
      "4972  yes, it sounds like yes, and, no, i don't mean...  \n",
      "4973  the pointlessness is grating. xi versions' fin...  \n",
      "4974  the basic ingredients are delicate, minimal, w...  \n",
      "4975  anyone bored by the kitchen sink will find muc...  \n",
      "4976  there are two vastly different mellencamps. on...  \n",
      "4977  the movie is incredible. everything about it i...  \n",
      "4978  houndmouth have the right touch and impressive...  \n",
      "4979  hawk is very much campbell's album. she made a...  \n",
      "4980  earle’s consistency is both his friend and his...  \n",
      "4981  think early air meets hip-hop, the west coast ...  \n",
      "4982  a certain kind of playfulness reigns throughou...  \n",
      "4983  one of the best country album ive heard in a w...  \n",
      "4984  what really shines through the most on magella...  \n",
      "4985  these songs are probably jurado's most ambitio...  \n",
      "4986  what's distinctive about son is that it is mor...  \n",
      "4987  at times this recording is compelling, entranc...  \n",
      "4988  unfortunately, more than mediocre tracks or th...  \n",
      "4989  a beautifully crafted debut album and one of t...  \n",
      "4990  a fine batch of bittersweet pop songs that are...  \n",
      "4991             the derivativeness quickly overwhelms.  \n",
      "4992  blue songs finds butler and his crew of collab...  \n",
      "4993  england's exoticism is offset by plenty of tou...  \n",
      "4994  weezer seems to have driven their old shtick i...  \n",
      "4995  as far as graduations from mixtapes to major-l...  \n",
      "4996  the result is an album that's unfortunately ba...  \n",
      "4997  in the end, island makes dave sound like he's ...  \n",
      "4998  beth ditto's remarkable gospel holler and ferv...  \n",
      "4999  dr. john is dr. john. he's a star, and is on f...  \n",
      "5000  their work, especially that displayed on refin...  \n",
      "\n",
      "[5001 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#first create a new column called \"body_tokens\" and transform to lowercase by applying the string function str.lower()\n",
    "df['body_tokens'] = df['body'].str.lower()\n",
    "\n",
    "#make sure it worked\n",
    "print(df[['body','body_tokens']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we tokenize the text. To do this on a Pandas dataframe we need the apply function. This simply tells the computer to take the function in the parentheses,, apply it to each row in the dataframe, and assign the output to a new column. \n",
    "\n",
    "There are two ways to do this. If it's a built-in function you're applying to the entire field, such as nltk.word_tokenize, you can simply put the function in the parentheses,. In some cases, you need to write your own function, called a lambda function. This is the case if you're applying something to a list (Pandas does not deal with list objects well. Hopefully someone smart will fix that). We'll get to that case below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [while, for, baltimore, proves, they, can, sti...\n",
      "1       [there, 's, nothing, fake, about, the, purgato...\n",
      "2       [all, life, 's, disastrous, lows, are, here, o...\n",
      "3       [with, doris, ,, odd, future’s, odysseus, is, ...\n",
      "4       [though, giraffe, is, definitely, echoboy, 's,...\n",
      "5       [fans, of, owl, city, and, the, postal, servic...\n",
      "6       [whereas, previous, elbow, records, set, a, mo...\n",
      "7       [his, remarkable, warp, debut, follows, a, ser...\n",
      "8       [there’s, an, energy, coursing, through, this,...\n",
      "9       [classic, ., songs, filled, with, soul, ., lyr...\n",
      "10      [it’s, by, no, means, perfect, and, it, does, ...\n",
      "11      [put, in, context, ,, white, chalk, serves, he...\n",
      "12      [although, pretty, catchy, ,, this, album, is,...\n",
      "13      [talk, about, a, fall, from, grace, ., [, 4, j...\n",
      "14      [it, 's, unusual, to, find, a, band, equally, ...\n",
      "15      [it, 's, just, a, shame, she, gave, album, spa...\n",
      "16      [the, fundamental, difference, between, the, m...\n",
      "17      [it, just, needs, to, be, a, passionate, ,, ca...\n",
      "18      [they, retained, their, best, ideas, for, them...\n",
      "19      [for, most, of, the, songs, amassed, here, ,, ...\n",
      "20      [laura, veirs, makes, an, excellent, case, for...\n",
      "21      [sure, ,, it, 's, beautiful, on, its, own, ,, ...\n",
      "22      [though, it, isn’t, a, concept, album, ,, live...\n",
      "23      [the, album, is, n't, without, its, problems––...\n",
      "24      [the, new, danger, is, as, overextended, as, i...\n",
      "25      [hebden, is, clearly, striving, for, dancefloo...\n",
      "26      [it, might, be, one, big, ,, saccharine, ,, ca...\n",
      "27      [the, cairo, gang, 's, superb, ,, weighty, ,, ...\n",
      "28      [as, snobbish, as, that, may, sound, ,, you, h...\n",
      "29      [any, number, of, tracks, here, could, easily,...\n",
      "                              ...                        \n",
      "4971    [it, is, an, interesting, experiment, ,, but, ...\n",
      "4972    [yes, ,, it, sounds, like, yes, ,, and, ,, no,...\n",
      "4973    [the, pointlessness, is, grating, ., xi, versi...\n",
      "4974    [the, basic, ingredients, are, delicate, ,, mi...\n",
      "4975    [anyone, bored, by, the, kitchen, sink, will, ...\n",
      "4976    [there, are, two, vastly, different, mellencam...\n",
      "4977    [the, movie, is, incredible, ., everything, ab...\n",
      "4978    [houndmouth, have, the, right, touch, and, imp...\n",
      "4979    [hawk, is, very, much, campbell, 's, album, .,...\n",
      "4980    [earle’s, consistency, is, both, his, friend, ...\n",
      "4981    [think, early, air, meets, hip-hop, ,, the, we...\n",
      "4982    [a, certain, kind, of, playfulness, reigns, th...\n",
      "4983    [one, of, the, best, country, album, ive, hear...\n",
      "4984    [what, really, shines, through, the, most, on,...\n",
      "4985    [these, songs, are, probably, jurado, 's, most...\n",
      "4986    [what, 's, distinctive, about, son, is, that, ...\n",
      "4987    [at, times, this, recording, is, compelling, ,...\n",
      "4988    [unfortunately, ,, more, than, mediocre, track...\n",
      "4989    [a, beautifully, crafted, debut, album, and, o...\n",
      "4990    [a, fine, batch, of, bittersweet, pop, songs, ...\n",
      "4991        [the, derivativeness, quickly, overwhelms, .]\n",
      "4992    [blue, songs, finds, butler, and, his, crew, o...\n",
      "4993    [england, 's, exoticism, is, offset, by, plent...\n",
      "4994    [weezer, seems, to, have, driven, their, old, ...\n",
      "4995    [as, far, as, graduations, from, mixtapes, to,...\n",
      "4996    [the, result, is, an, album, that, 's, unfortu...\n",
      "4997    [in, the, end, ,, island, makes, dave, sound, ...\n",
      "4998    [beth, ditto, 's, remarkable, gospel, holler, ...\n",
      "4999    [dr., john, is, dr., john, ., he, 's, a, star,...\n",
      "5000    [their, work, ,, especially, that, displayed, ...\n",
      "Name: body_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#tokenize\n",
    "df['body_tokens'] = df['body_tokens'].apply(nltk.word_tokenize)\n",
    "\n",
    "#view output\n",
    "print(df['body_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [while, for, baltimore, proves, they, can, sti...\n",
      "1       [there, 's, nothing, fake, about, the, purgato...\n",
      "2       [all, life, 's, disastrous, lows, are, here, o...\n",
      "3       [with, doris, odd, future’s, odysseus, is, fin...\n",
      "4       [though, giraffe, is, definitely, echoboy, 's,...\n",
      "5       [fans, of, owl, city, and, the, postal, servic...\n",
      "6       [whereas, previous, elbow, records, set, a, mo...\n",
      "7       [his, remarkable, warp, debut, follows, a, ser...\n",
      "8       [there’s, an, energy, coursing, through, this,...\n",
      "9       [classic, songs, filled, with, soul, lyrics, r...\n",
      "10      [it’s, by, no, means, perfect, and, it, does, ...\n",
      "11      [put, in, context, white, chalk, serves, her, ...\n",
      "12      [although, pretty, catchy, this, album, is, a,...\n",
      "13      [talk, about, a, fall, from, grace, 4, jun, 20...\n",
      "14      [it, 's, unusual, to, find, a, band, equally, ...\n",
      "15      [it, 's, just, a, shame, she, gave, album, spa...\n",
      "16      [the, fundamental, difference, between, the, m...\n",
      "17      [it, just, needs, to, be, a, passionate, catha...\n",
      "18      [they, retained, their, best, ideas, for, them...\n",
      "19      [for, most, of, the, songs, amassed, here, it,...\n",
      "20      [laura, veirs, makes, an, excellent, case, for...\n",
      "21      [sure, it, 's, beautiful, on, its, own, but, w...\n",
      "22      [though, it, isn’t, a, concept, album, live, a...\n",
      "23      [the, album, is, n't, without, its, problems––...\n",
      "24      [the, new, danger, is, as, overextended, as, i...\n",
      "25      [hebden, is, clearly, striving, for, dancefloo...\n",
      "26      [it, might, be, one, big, saccharine, catchy, ...\n",
      "27      [the, cairo, gang, 's, superb, weighty, meticu...\n",
      "28      [as, snobbish, as, that, may, sound, you, have...\n",
      "29      [any, number, of, tracks, here, could, easily,...\n",
      "                              ...                        \n",
      "4971    [it, is, an, interesting, experiment, but, rea...\n",
      "4972    [yes, it, sounds, like, yes, and, no, i, do, n...\n",
      "4973    [the, pointlessness, is, grating, xi, versions...\n",
      "4974    [the, basic, ingredients, are, delicate, minim...\n",
      "4975    [anyone, bored, by, the, kitchen, sink, will, ...\n",
      "4976    [there, are, two, vastly, different, mellencam...\n",
      "4977    [the, movie, is, incredible, everything, about...\n",
      "4978    [houndmouth, have, the, right, touch, and, imp...\n",
      "4979    [hawk, is, very, much, campbell, 's, album, sh...\n",
      "4980    [earle’s, consistency, is, both, his, friend, ...\n",
      "4981    [think, early, air, meets, hip-hop, the, west,...\n",
      "4982    [a, certain, kind, of, playfulness, reigns, th...\n",
      "4983    [one, of, the, best, country, album, ive, hear...\n",
      "4984    [what, really, shines, through, the, most, on,...\n",
      "4985    [these, songs, are, probably, jurado, 's, most...\n",
      "4986    [what, 's, distinctive, about, son, is, that, ...\n",
      "4987    [at, times, this, recording, is, compelling, e...\n",
      "4988    [unfortunately, more, than, mediocre, tracks, ...\n",
      "4989    [a, beautifully, crafted, debut, album, and, o...\n",
      "4990    [a, fine, batch, of, bittersweet, pop, songs, ...\n",
      "4991           [the, derivativeness, quickly, overwhelms]\n",
      "4992    [blue, songs, finds, butler, and, his, crew, o...\n",
      "4993    [england, 's, exoticism, is, offset, by, plent...\n",
      "4994    [weezer, seems, to, have, driven, their, old, ...\n",
      "4995    [as, far, as, graduations, from, mixtapes, to,...\n",
      "4996    [the, result, is, an, album, that, 's, unfortu...\n",
      "4997    [in, the, end, island, makes, dave, sound, lik...\n",
      "4998    [beth, ditto, 's, remarkable, gospel, holler, ...\n",
      "4999    [dr., john, is, dr., john, he, 's, a, star, an...\n",
      "5000    [their, work, especially, that, displayed, on,...\n",
      "Name: body_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "punctuations = list(string.punctuation)\n",
    "\n",
    "#remove punctuation. Let's talk about that lambda x.\n",
    "df['body_tokens'] = df['body_tokens'].apply(lambda x: [word for word in x if word not in punctuations])\n",
    "\n",
    "#view output\n",
    "print(df['body_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing is done. What other pre-processing steps might we use?\n",
    "\n",
    "One more step before getting to the dictionary method. We want a total token count for each row, so we can normalize the dictionary counts. To do this we simply create a new column that contains the length of the token list in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            body_tokens  token_count\n",
      "0     [while, for, baltimore, proves, they, can, sti...           40\n",
      "1     [there, 's, nothing, fake, about, the, purgato...           29\n",
      "2     [all, life, 's, disastrous, lows, are, here, o...           14\n",
      "3     [with, doris, odd, future’s, odysseus, is, fin...           16\n",
      "4     [though, giraffe, is, definitely, echoboy, 's,...           51\n",
      "5     [fans, of, owl, city, and, the, postal, servic...           34\n",
      "6     [whereas, previous, elbow, records, set, a, mo...           34\n",
      "7     [his, remarkable, warp, debut, follows, a, ser...           21\n",
      "8     [there’s, an, energy, coursing, through, this,...           38\n",
      "9     [classic, songs, filled, with, soul, lyrics, r...           60\n",
      "10    [it’s, by, no, means, perfect, and, it, does, ...           20\n",
      "11    [put, in, context, white, chalk, serves, her, ...           47\n",
      "12    [although, pretty, catchy, this, album, is, a,...           10\n",
      "13    [talk, about, a, fall, from, grace, 4, jun, 20...           10\n",
      "14    [it, 's, unusual, to, find, a, band, equally, ...           27\n",
      "15    [it, 's, just, a, shame, she, gave, album, spa...           25\n",
      "16    [the, fundamental, difference, between, the, m...           31\n",
      "17    [it, just, needs, to, be, a, passionate, catha...           21\n",
      "18    [they, retained, their, best, ideas, for, them...           22\n",
      "19    [for, most, of, the, songs, amassed, here, it,...           54\n",
      "20    [laura, veirs, makes, an, excellent, case, for...           28\n",
      "21    [sure, it, 's, beautiful, on, its, own, but, w...           28\n",
      "22    [though, it, isn’t, a, concept, album, live, a...           31\n",
      "23    [the, album, is, n't, without, its, problems––...           43\n",
      "24    [the, new, danger, is, as, overextended, as, i...           10\n",
      "25    [hebden, is, clearly, striving, for, dancefloo...           35\n",
      "26    [it, might, be, one, big, saccharine, catchy, ...           31\n",
      "27    [the, cairo, gang, 's, superb, weighty, meticu...           18\n",
      "28    [as, snobbish, as, that, may, sound, you, have...           21\n",
      "29    [any, number, of, tracks, here, could, easily,...           20\n",
      "...                                                 ...          ...\n",
      "4971  [it, is, an, interesting, experiment, but, rea...           27\n",
      "4972  [yes, it, sounds, like, yes, and, no, i, do, n...           19\n",
      "4973  [the, pointlessness, is, grating, xi, versions...           45\n",
      "4974  [the, basic, ingredients, are, delicate, minim...           22\n",
      "4975  [anyone, bored, by, the, kitchen, sink, will, ...           15\n",
      "4976  [there, are, two, vastly, different, mellencam...           29\n",
      "4977  [the, movie, is, incredible, everything, about...          105\n",
      "4978  [houndmouth, have, the, right, touch, and, imp...           33\n",
      "4979  [hawk, is, very, much, campbell, 's, album, sh...           36\n",
      "4980  [earle’s, consistency, is, both, his, friend, ...           29\n",
      "4981  [think, early, air, meets, hip-hop, the, west,...           22\n",
      "4982  [a, certain, kind, of, playfulness, reigns, th...           11\n",
      "4983  [one, of, the, best, country, album, ive, hear...           85\n",
      "4984  [what, really, shines, through, the, most, on,...           57\n",
      "4985  [these, songs, are, probably, jurado, 's, most...           16\n",
      "4986  [what, 's, distinctive, about, son, is, that, ...           20\n",
      "4987  [at, times, this, recording, is, compelling, e...           16\n",
      "4988  [unfortunately, more, than, mediocre, tracks, ...           65\n",
      "4989  [a, beautifully, crafted, debut, album, and, o...           53\n",
      "4990  [a, fine, batch, of, bittersweet, pop, songs, ...           13\n",
      "4991         [the, derivativeness, quickly, overwhelms]            4\n",
      "4992  [blue, songs, finds, butler, and, his, crew, o...           33\n",
      "4993  [england, 's, exoticism, is, offset, by, plent...           28\n",
      "4994  [weezer, seems, to, have, driven, their, old, ...           24\n",
      "4995  [as, far, as, graduations, from, mixtapes, to,...           32\n",
      "4996  [the, result, is, an, album, that, 's, unfortu...           27\n",
      "4997  [in, the, end, island, makes, dave, sound, lik...           17\n",
      "4998  [beth, ditto, 's, remarkable, gospel, holler, ...           26\n",
      "4999  [dr., john, is, dr., john, he, 's, a, star, an...           18\n",
      "5000  [their, work, especially, that, displayed, on,...           28\n",
      "\n",
      "[5001 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df['token_count'] = df['body_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "print(df[['body_tokens','token_count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating Dictionary Counts\n",
    "\n",
    "I created two text files, one is a list of positive words from the MPQA dictionary, the other is a list of negative words. One word per line. Our goal here is to count the number of positive and negative words in each row of our dataframe, and add two columns to our dataset with the count of positive and negative words.\n",
    "\n",
    "First, read in the positive and negative words and create list variables for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abidance\n",
      "abidance\n",
      "abilities\n",
      "ability\n",
      "able\n",
      "above\n",
      "above-average\n",
      "abundant\n",
      "abundance\n",
      "acceptance\n",
      "acceptable\n"
     ]
    }
   ],
   "source": [
    "pos_sent = open(\"positive_words.txt\").read()\n",
    "neg_sent = open(\"negative_words.txt\").read()\n",
    "\n",
    "#view part of the pos_sent variable, to see how it's formatted.\n",
    "print(pos_sent[:101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abidance', 'abidance', 'abilities', 'ability', 'able', 'above', 'above-average', 'abundant', 'abundance', 'acceptance']\n",
      "['abandoned', 'abandonment', 'aberration', 'aberration', 'abhorred', 'abhorrence', 'abhorrent', 'abhorrently', 'abhors', 'abhors']\n"
     ]
    }
   ],
   "source": [
    "#remember the split function? We'll split on the newline character (\\n) to create a list\n",
    "positive_words=pos_sent.split('\\n')\n",
    "negative_words=neg_sent.split('\\n')\n",
    "\n",
    "#view the first elements in the lists\n",
    "print(positive_words[:10])\n",
    "print(negative_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2231\n",
      "3906\n"
     ]
    }
   ],
   "source": [
    "#count number of words in each list\n",
    "print(len(positive_words))\n",
    "print(len(negative_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we can create two more columns that contain the number of positive words and negative words in the review tokens. I'm going to get creative with this, as we need to do this step in one line of code for positive and negative words, each. Your challenges:\n",
    "\n",
    "* Can you parse the code? We'll walk through it together.\n",
    "* Think of other ways you could do this same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      token_count  positive_tokens  negative_tokens\n",
      "0              40                1                0\n",
      "1              29                0                3\n",
      "2              14                0                1\n",
      "3              16                0                1\n",
      "4              51                2                4\n",
      "5              34                4                0\n",
      "6              34                2                0\n",
      "7              21                3                0\n",
      "8              38                1                0\n",
      "9              60                8                0\n",
      "10             20                3                1\n",
      "11             47                1                1\n",
      "12             10                2                1\n",
      "13             10                1                0\n",
      "14             27                2                1\n",
      "15             25                2                2\n",
      "16             31                2                2\n",
      "17             21                2                1\n",
      "18             22                1                1\n",
      "19             54                2                2\n",
      "20             28                1                0\n",
      "21             28                2                0\n",
      "22             31                2                0\n",
      "23             43                2                1\n",
      "24             10                0                1\n",
      "25             35                2                2\n",
      "26             31                3                0\n",
      "27             18                3                1\n",
      "28             21                2                0\n",
      "29             20                0                0\n",
      "...           ...              ...              ...\n",
      "4971           27                3                1\n",
      "4972           19                1                1\n",
      "4973           45                0                2\n",
      "4974           22                2                0\n",
      "4975           15                2                0\n",
      "4976           29                1                2\n",
      "4977          105               10                0\n",
      "4978           33                4                0\n",
      "4979           36                0                0\n",
      "4980           29                3                2\n",
      "4981           22                1                0\n",
      "4982           11                1                0\n",
      "4983           85                2                2\n",
      "4984           57                4                3\n",
      "4985           16                1                0\n",
      "4986           20                2                0\n",
      "4987           16                4                1\n",
      "4988           65                4                2\n",
      "4989           53                4                0\n",
      "4990           13                1                1\n",
      "4991            4                0                0\n",
      "4992           33                1                1\n",
      "4993           28                3                1\n",
      "4994           24                2                2\n",
      "4995           32                1                0\n",
      "4996           27                0                3\n",
      "4997           17                3                0\n",
      "4998           26                2                0\n",
      "4999           18                1                0\n",
      "5000           28                5                0\n",
      "\n",
      "[5001 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#create column with the number of positive words\n",
    "df['positive_tokens'] = df['body_tokens'].apply(lambda x: len([word for word in x if word in positive_words]))\n",
    "df['negative_tokens'] = df['body_tokens'].apply(lambda x: len([word for word in x if word in negative_words]))\n",
    "\n",
    "print(df[['token_count', 'positive_tokens', 'negative_tokens']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the dictionary method! You can do this with any dictionary you want, standard or you can create your own.\n",
    "\n",
    "### 2. Sentiment Analysis using the Dictionary Method\n",
    "\n",
    "What can we do with this?\n",
    "\n",
    "First, let's compare the overall sentiment of the reviews by genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion Positive Words\n",
      "genre\n",
      "Jazz                      0.085170\n",
      "Folk                      0.084559\n",
      "Alternative/Indie Rock    0.073557\n",
      "Indie                     0.073287\n",
      "Rock                      0.071613\n",
      "Electronic                0.070994\n",
      "Pop/Rock                  0.070922\n",
      "Pop                       0.069388\n",
      "R&B;                      0.069345\n",
      "Country                   0.061577\n",
      "Dance                     0.061299\n",
      "Rap                       0.060397\n",
      "dtype: float64\n",
      "\n",
      "Proportion Negative Words\n",
      "genre\n",
      "R&B;                      0.023390\n",
      "Jazz                      0.025050\n",
      "Folk                      0.026961\n",
      "Pop                       0.028912\n",
      "Country                   0.031204\n",
      "Pop/Rock                  0.031931\n",
      "Alternative/Indie Rock    0.032588\n",
      "Rock                      0.033084\n",
      "Rap                       0.033481\n",
      "Electronic                0.033514\n",
      "Indie                     0.033866\n",
      "Dance                     0.034767\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#use groupby function\n",
    "df_genres = df.groupby('genre')\n",
    "\n",
    "print(\"Proportion Positive Words\")\n",
    "print((df_genres['positive_tokens'].sum()/df_genres['token_count'].sum()).sort_values(ascending=False))\n",
    "print()\n",
    "print(\"Proportion Negative Words\")\n",
    "print((df_genres['negative_tokens'].sum()/df_genres['token_count'].sum()).sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the position of Rap and R&B; in both lists. These lists are not inverses. This suggests positive and negative emotion words are in some cases orthogonal. \n",
    "\n",
    "Compare these lists to the average score by genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre\n",
      "Jazz                      77.631579\n",
      "Folk                      75.900000\n",
      "Indie                     74.400897\n",
      "Country                   74.071429\n",
      "Alternative/Indie Rock    73.928571\n",
      "Electronic                73.140351\n",
      "Pop/Rock                  73.033782\n",
      "R&B;                      72.366071\n",
      "Rap                       72.173554\n",
      "Rock                      70.754292\n",
      "Dance                     70.146341\n",
      "Pop                       64.608054\n",
      "Name: score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_genres['score'].mean().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the position of Country in both lists. What might you conclude from this?\n",
    "\n",
    "As another validation check, let's groupby score and see if that matches intuitively with our emotion proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of Postive Words\n",
      "High Score:\n",
      "0.0729579698652\n",
      "Low Score:\n",
      "0.0664526603203\n",
      "\n",
      "Proportion of Negative Words\n",
      "High Score:\n",
      "0.0317625944322\n",
      "Low Score:\n",
      "0.0362703859494\n"
     ]
    }
   ],
   "source": [
    "df_highscore = df[df['score']>=75]\n",
    "df_lowscore = df[df['score']<65]\n",
    "print(\"Proportion of Postive Words\")\n",
    "print(\"High Score:\")\n",
    "print((df_highscore['positive_tokens'].sum()/df_highscore['token_count'].sum()))\n",
    "print(\"Low Score:\")\n",
    "print((df_lowscore['positive_tokens'].sum()/df_lowscore['token_count'].sum()))\n",
    "print()\n",
    "print(\"Proportion of Negative Words\")\n",
    "print(\"High Score:\")\n",
    "print((df_highscore['negative_tokens'].sum()/df_highscore['token_count'].sum()))\n",
    "print(\"Low Score:\")\n",
    "print((df_lowscore['negative_tokens'].sum()/df_lowscore['token_count'].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. But this also illustrates potential problems with sentiment analysis, and the dictionary method in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: \n",
    "* Make your own dictionary of terms as a text file, each word on a separate line. Reproduce this analysis using your own dictionary.\n",
    "* Reproduce this on your own corpus, or one of our other corpuses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
