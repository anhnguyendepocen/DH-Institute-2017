{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>It Starts with a Research Question...</h1>\n",
    "<img src='Long, So 263, Fig 8.png' width=\"66%\" height=\"66%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literary Distinction (Probably)\n",
    "<ul><li>Preview</li>\n",
    "<li>Review</li>\n",
    "<li>Pre-Processing</li>\n",
    "<ul>\n",
    "<li>Import Corpus</li>\n",
    "<li>Stop Words</li>\n",
    "<li>Feature Selection</li></ul>\n",
    "<li>Classification</li>\n",
    "<ul>\n",
    "<li>Training, Feature Importance, & Prediction</li>\n",
    "<li>Literary Distinction</li>\n",
    "<li>Extra: Cross-Validation</li></ul>\n",
    "</ul>\n",
    "\n",
    "Although Long and So's study of modernist haiku motivates this lesson, a substantial portion of their corpus remains under copyright so they have not made it available publicly. Instead we will apply their methods to the corpus distributed by Ted Underwood and Jordan Sellers in support of their own literary historical study on nineteenth- and early-twentieth century volumes of poetry that were reviewed in prestigious magazines versus not at all. (The idea being that even a negative review indicates valuable, critical engagement.)\n",
    "\n",
    "In essence, our task will be to learn the vocabulary of literary prestige, rather than that of haiku. We will however be deliberate in using Long and So's methods, since they reflect assumptions about language that are more appropriate to a general introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get texts of interest that belong to identifiably different categories\n",
    "\n",
    "unladen_swallow = 'high air-speed velocity'\n",
    "swallow_grasping_coconut = 'low air-speed velocity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform them into a format scikit-learn can use\n",
    "\n",
    "columns = ['high','low','air-speed','velocity']\n",
    "indices = ['unladen', 'coconut']\n",
    "dtm = [[1,0,1,1],[0,1,1,1]]\n",
    "dtm_df = pandas.DataFrame(dtm, columns = columns, index = indices)\n",
    "\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the Naive Bayes classifier\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(dtm,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a prediction!\n",
    "\n",
    "unknown_swallow = \"high velocity\"\n",
    "unknown_features = [1,0,0,1]\n",
    "\n",
    "nb.predict(unknown_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read Moby Dick\n",
    "moby_string = open('Melville - Moby Dick.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect the text\n",
    "moby_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make the text lower case\n",
    "moby_lower = moby_string.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize Moby Dick\n",
    "\n",
    "moby_tokens = moby_lower.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check out the tokens\n",
    "moby_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just how long is Moby Dick anyway?\n",
    "len(moby_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list comprehension, including an 'if' statement\n",
    "just_whales = [token for token in moby_tokens if token=='whale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hast seen the White Whale?\n",
    "just_whales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maritime = ['ship','harpoon','sail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maritime * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whaling = ['whiteness','whale','ambergris']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maritime + whaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pre-Process\n",
    "\n",
    "In their paper, Long and So describe their pre-processing as consisting of three major steps: stop word removal, lemmatization of nouns, and feature selection (based on document frequency). In this workshop, we will focus on the first and third steps, since they can be integrated seamlessly with our workflow and Underwood and Sellers use them as well.\n",
    "\n",
    "Lemmatization -- the transformation of words into their dictionary forms; e.g. plural nouns become singular -- is particularly useful to Long and So, since they aim partly to study imagery. For example, they find it beneficial to collapse <i>mountains</i> and <i>mountain</i> into the same token. For an introduction to Lemmatization (and a related technique, Stemming), see NLTK: http://www.nltk.org/book/ch03.html#sec-normalizing-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Corpus\n",
    "\n",
    "Note that due to issues of copyright, volumes' word order has not been retained, although their total word counts have been. Fortunately, our methods do not require word-order information.\n",
    "\n",
    "Underwood and Sellers's literary corpus has been divided into three folders: \"reviewed\", \"random\", \"canonic\". (The last of these are canonic poets but who did not have the opportunity to be reviewed, such as Emily Dickinson.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign file paths to each set of poems\n",
    "\n",
    "review_path = 'poems/reviewed/'\n",
    "random_path = 'poems/random/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get lists of text files in each directory\n",
    "\n",
    "review_files = os.listdir(review_path)\n",
    "random_files = os.listdir(random_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "review_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in texts as strings from each location\n",
    "\n",
    "review_texts = [open(review_path+file_name).read() for file_name in review_files]\n",
    "random_texts = [open(random_path+file_name).read() for file_name in random_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "review_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collect all texts in single list\n",
    "\n",
    "all_texts = review_texts + random_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all file names together\n",
    "\n",
    "all_file_names = review_files + random_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep track of classes with labels\n",
    "\n",
    "all_labels = ['reviewed'] * len(review_texts) + ['random'] * len(random_texts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. How many file names are listed in the directory for reviewed texts? \n",
    "\n",
    "## EX. How many texts got read into 'review_texts'? Does it match the number of files in the directory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "\n",
    "<i>Stop words</i>, sometimes refered to as <i>function words</i>, include articles, prepositions, pronouns, and conjunctions among others. Although their frequencies encode information about textual features like authorship, they do not convey semantic meanings and are often removed before analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# By default scikit-learn uses this list of English stop words\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many are here?\n",
    "\n",
    "len(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK has its own collection of stop words\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pull up NLTK's list of English-language stop words\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many stop words are in the list?\n",
    "\n",
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NLTK has stopwords for many Western languages\n",
    "\n",
    "stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_sentence = ['what', 'is', 'the', 'air-speed', 'velocity', 'of', 'an', 'unladen', 'swallow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove stopwords from tokenized sentence\n",
    "\n",
    "[word for word in tokenized_sentence if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Q.  Stop words are typically the most frequent words in a language, yet do not convey semantic meaning.\n",
    "##     Does this make sense based on the words in NLTK's list of English stop words?\n",
    "##     What about other languages with which you are familar?\n",
    "\n",
    "stopword_languages = ['danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian',\\\n",
    "                      'norwegian', 'portuguese', 'russian', 'spanish', 'swedish', 'turkish']\n",
    "\n",
    "## EX. Use either sklearn or NLTK's stopword list to remove those words from the 'token_list' below.\n",
    "\n",
    "## EX. How many tokens did you remove from the 'token_list' in total? What percent were removed?\n",
    "\n",
    "## CHALLENGE.  Stop words are often instrumental in language detection for unknown texts.\n",
    "##             How might you write a program to do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_list = ['in', 'a', 'station', 'of', 'the', 'metro',\\\n",
    "              'the', 'apparition', 'of', 'these', 'faces', 'in', 'the', 'crowd',\\\n",
    "              'petals', 'on', 'a', 'wet', 'black', 'bough']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "At this point, we transform our texts into a Document-Term Matrix in the same manner we have employed previously. However, it is important to note that neither Long and So nor Underwood and Sellers use all of the words that appear in their respective corpora when constructing their matrices. The process of choosing which words to comprise the columns is referred to as <i>feature selection</i>.\n",
    "\n",
    "While there are several approaches one may take when selecting features, both of the literary studies under consideration use <i>document frequency</i> as the deciding criterion. The intuition is that a word that appears in a single text out of hundreds will not carry much weight when trying to determine the text's class membership.\n",
    "\n",
    "In order to be selected as a feature, Long and So require that words appear in at least 2 texts, whereas Underwood and Sellers require that a word appear in about a quarter of all texts. Although this is quite a large difference (a minimum of 2 texts vs. ~180 texts), it perhaps makes sense since the texts are of very different lengths: individual haiku vs entire volumes of poetry. The latter will have much greater overlap in its vocabulary.\n",
    "\n",
    "The process of feature selection is intimately tied to the object under study and the statistical model chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Intitialize the function that will transform our list of texts to a DTM\n",
    "# 'min_df' and 'max_features' are arguments that enable flexible feature selection\n",
    "# 'binary' tells CountVectorizer only to record whether a word appeared in a text or not\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', min_df=180, binary = True, max_features = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform our texts to DTM\n",
    "\n",
    "cv.fit_transform(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform our texts to a dense DTM\n",
    "\n",
    "cv.fit_transform(all_texts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign this to a variable\n",
    "\n",
    "dtm = cv.fit_transform(all_texts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the column headings\n",
    "\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign to a variable\n",
    "\n",
    "feature_list = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Place this in a dataframe for readability\n",
    "\n",
    "dtm_df = pandas.DataFrame(dtm, columns = feature_list, index = all_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check out the dataframe\n",
    "\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the dataframe's dimensions (# texts, # features)\n",
    "\n",
    "dtm_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Re-initialize the CountVectorizer function above with the the argument min_df = 1.\n",
    "##     How many unique words are in there in the total vocabulary of the corpus?\n",
    "\n",
    "## EX. Repeat the exercise above with min_df = 360. (That is, words are only included if they appear\n",
    "##     in at least half of all documents.) What is the size of the vocabulary now?\n",
    "##     Does the list of these very common words look like you would expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Feature Importance, and Prediction\n",
    "\n",
    "We have selected an algorithm that specifically relies on <a href=\"https://en.wikipedia.org/wiki/Bayes%27_theorem\">Bayes' Theorem</a> to model relationships between textual features and categories in our corpus of poetry volumes. (See link for more information about the method and its assumptions.)\n",
    "\n",
    "Two ways that we learn about the model are its feature weights and predictions on new texts. The algorithm can explicity report to us which direction each word leans category-wise and how strongly. Based on those weights, it makes further predictions about the valences of previously unseen poetry volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the classifier and assign it to a variable\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(dtm, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hand-waving the underlying statistics here...\n",
    "\n",
    "def most_informative_features(text_class, vectorizer = cv, classifier = nb, top_n = 10):\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    class_index = np.where(classifier.classes_==(text_class))[0][0]\n",
    "    \n",
    "    class_prob_distro = np.exp(classifier.feature_log_prob_[class_index])\n",
    "    alt_class_prob_distro = np.exp(classifier.feature_log_prob_[1 - class_index])\n",
    "    \n",
    "    odds_ratios = class_prob_distro / alt_class_prob_distro\n",
    "    odds_with_fns = sorted(zip(odds_ratios, feature_names), reverse = True)\n",
    "    \n",
    "    return odds_with_fns[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns feature name and odds ratio for a given class\n",
    "\n",
    "most_informative_features('reviewed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Similarly, for words that indicate 'negative' class membership\n",
    "\n",
    "most_informative_features('random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's load up two poems that aren't in the training set and make predictions\n",
    "\n",
    "dickinson_canonic = \"\"\"Because I could not stop for Death – \n",
    "He kindly stopped for me –  \n",
    "The Carriage held but just Ourselves –  \n",
    "And Immortality.\n",
    "\n",
    "We slowly drove – He knew no haste\n",
    "And I had put away\n",
    "My labor and my leisure too,\n",
    "For His Civility – \n",
    "\n",
    "We passed the School, where Children strove\n",
    "At Recess – in the Ring –  \n",
    "We passed the Fields of Gazing Grain –  \n",
    "We passed the Setting Sun – \n",
    "\n",
    "Or rather – He passed us – \n",
    "The Dews drew quivering and chill – \n",
    "For only Gossamer, my Gown – \n",
    "My Tippet – only Tulle – \n",
    "\n",
    "We paused before a House that seemed\n",
    "A Swelling of the Ground – \n",
    "The Roof was scarcely visible – \n",
    "The Cornice – in the Ground – \n",
    "\n",
    "Since then – ‘tis Centuries – and yet\n",
    "Feels shorter than the Day\n",
    "I first surmised the Horses’ Heads \n",
    "Were toward Eternity – \"\"\"\n",
    "\n",
    "\n",
    "anthem_patriotic = \"\"\"O! say can you see, by the dawn's early light,\n",
    "What so proudly we hailed at the twilight's last gleaming,\n",
    "Whose broad stripes and bright stars through the perilous fight,\n",
    "O'er the ramparts we watched, were so gallantly streaming?\n",
    "And the rockets' red glare, the bombs bursting in air,\n",
    "Gave proof through the night that our flag was still there;\n",
    "O! say does that star-spangled banner yet wave\n",
    "O'er the land of the free and the home of the brave?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform these into DTMs with the same feature-columns as previously\n",
    "\n",
    "unknown_dtm = cv.transform([dickinson_canonic,anthem_patriotic]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What does the classifier think?\n",
    "\n",
    "nb.predict(unknown_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Although our classification is binary, Bayes theorem assigns\n",
    "# a probability of membership in either category\n",
    "\n",
    "# Just how confident is our classifier of its predictions?\n",
    "\n",
    "nb.predict_proba(unknown_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Q.  What kinds of patterns do you notice among the 'most informative features'?\n",
    "##     Try looking at the top fifty most informative words for each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Literary Distinction\n",
    "\n",
    "In their study of critical taste, Underwood and Sellers find not only that literary standards change very slowly, but that contemporary evaluations of 'canonicity' resemble those of the nineteenth century.\n",
    "\n",
    "In order to test this idea, the authors trained a classifier on nineteenth- and early twentieth-century volumes of poetry that received reviews in a prestigious magazine versus those that didn't. The authors then used the classifier to predict a category for volumes of poetry that went unreviewed, in several cases because they were unpublished, but are now included in Norton anthologies.\n",
    "\n",
    "How closely does critical evaluation today match that of a century ago?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Import and process the 'canonic' (albeit unreviewed) volumes of poetry.\n",
    "##     Use the poetry classifier to predict whether they might have been reviewed.\n",
    "##     Does the output make sense? Is it consistent with Underwood and Sellers's findings?\n",
    "\n",
    "canonic_path = 'poems/canonic/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Cross-Validation\n",
    "\n",
    "Just how good is our classifier? We can evaluate it by randomly selecting texts from each category and setting them aside before training. We then see how well the classifier predicts their (known) categories.\n",
    "\n",
    "Remember that if the classifier is trying to predict membership for just two categories, we would expect it to be correct about 50% of the time based on random chance. As a rule of thumb, if this kind of classifier has 65% accuracy or better under cross-validation, it has often identified a meaningful pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Randomize the order of our texts\n",
    "import numpy\n",
    "randomized_review = numpy.random.permutation(review_texts)\n",
    "randomized_random = numpy.random.permutation(random_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll train our classifier on the first 90% of texts in the randomized list\n",
    "# Then, we'll test it using the last 10%\n",
    "\n",
    "training_set = list(randomized_review[:324]) + list(randomized_random[:324])\n",
    "test_set = list(randomized_review[324:]) + list(randomized_random[324:])\n",
    "\n",
    "training_labels = ['reviewed'] * 324 + ['random'] * 324\n",
    "test_labels = ['reviewed'] * 36 + ['random'] * 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform training and test texts into DTMs\n",
    "# Note that 'min_df' has been adjusted to one quarter of the size of the training set\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', min_df = 162, binary=True)\n",
    "training_dtm = cv.fit_transform(training_set)\n",
    "test_dtm = cv.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train, Predict, Evaluate\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(training_dtm, training_labels)\n",
    "predictions = nb.predict(test_dtm)\n",
    "accuracy_score(predictions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CHALLENGE: In fact, when Underwood and Sellers cross-validate, they do so by setting aside a single\n",
    "##            author's texts (one or more) from the training set and making a prediction for that author alone.\n",
    "##            After doing this for all authors, they tally the number of texts that were correctly predicted\n",
    "##            to calculate their overall accuracy. Implement this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
