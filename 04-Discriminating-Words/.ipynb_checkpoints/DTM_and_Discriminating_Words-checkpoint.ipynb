{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Document Term Matrix and Discriminating Words\n",
    "\n",
    "The Document Term Matrix (DTM) in the bread and butter of most computational text analysis techniques, both simple and more sophisticated methods. In this lesson we will use Python's scikit-learn package learn to make a document term matrix from the .csv Music Reviews dataset. We will then use the DTM and a word weighting technique called tf-idf (term frequency inverse document frequency) to identify important and discriminating words within this dataset (utilizing the Pandas package). The illustrating question: what words distinguish reviews of Rap albums, Indie Rock albums, and Jazz albums? Theoretical exerxise: What can we learn from these words?\n",
    "\n",
    "Note: Python's scikit-learn package is an enormous package with a lot of functionality. Knowing this package will enable you to do some very sophisticated analyses, including almost all machine learning techniques. (It looks great on your CV too!). We'll get back to this package later in the workshop.\n",
    "\n",
    "### Learning Goals\n",
    "* Understand the DTM and why it's important to text analysis\n",
    "* Learn how to create a DTM from a .csv file\n",
    "* Learn basic functionality of Python's package scikit-learn (we'll return to scikit-learn in lesson 06)\n",
    "* Understand tf-idf scores, and word scores in general\n",
    "* Learn a simple way to identify distinctive words\n",
    "* In the process, gain more familiarity and comfort with the Pandas pacakge and manipulating data\n",
    "\n",
    "### Outline\n",
    "* The Pandas Dataframe: Music Reviews\n",
    "* Explore the Data using Pandas\n",
    "    * Basic descriptive statistics\n",
    "    * The Groupby function\n",
    "* Creating the DTM: scikit-learn\n",
    "    * CountVectorizer function\n",
    "* Tf-idf scores\n",
    "    * TfidfVectorizer\n",
    "*  Identifying Distinctive Words\n",
    "    * Identify distinctive reviews by genre\n",
    "\n",
    "\n",
    "### Key Jargon\n",
    "* *Document Term Matrix*:\n",
    "    * a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms.\n",
    "* *TF-IDF Scores*: \n",
    "    *  short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "    \n",
    "### Further Resources\n",
    "\n",
    "[This blog post](https://de.dariah.eu/tatom/feature_selection.html) goes through finding distinctive words using Python in more detail \n",
    "\n",
    "Paper: [Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf), Burt Monroe, Michael Colaresi, Kevin Quinn\n",
    "    \n",
    "### 0. The Pandas Dataframe: Music Reviews\n",
    "\n",
    "First, we read our music reviews corpus, which is stored as a .csv file on our hard drive, into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "#create a dataframe called \"df\"\n",
    "df = pandas.read_csv(\"BDHSI2016_music_reviews.csv\", sep = '\\t')\n",
    "\n",
    "##I'm going to do a pre-processing step to remove digits in the text, for analytical purposes.\n",
    "##If you don't understand this code right now it's ok. But challenge yourself to make sense of it!\n",
    "df['body'] = df['body'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))\n",
    "\n",
    "#view the dataframe\n",
    "#notice the metadata. The column \"body\" contains our text of interest.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Review Ex: Think back to yesterday's tutorial on Pandas.\n",
    "###Use the dataframe slicing methods to print the full text of the first review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explore the Data using Pandas\n",
    "\n",
    "Let's first look at some descriptive statistics about this dataset, to get a feel for what's in it. We'll do this using the Pandas package. \n",
    "\n",
    "Note: this is always good practice. It serves two purposes. It checks to make sure your data is correct, and there's no major errors. It also keeps you in touch with your data, which will help with interpretation. <3 your data!\n",
    "\n",
    "There are a number of built-in functions in the Pandas package which makes summarizing, manipulating, and visualizing your data easy. That's the point of Pandas!\n",
    "\n",
    "#### Summarizing your Data\n",
    "\n",
    "We'll start with summarizing our data. The most basic way to summarize your data is using the describe function. The syntax is simply `DataFrame.describe()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this provides summary statistics for numerical columns. In our case, our only numerical column is `score`. We can access each of these summaries statistics separately, using Pandas built-in functions. For example, we can get the max score using the syntax `DataFrame[Column Name].max()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['score'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##EX: Print the mean and the standard deviation for the score column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the other columns? The columns that contain strings, not numbers? One way we can summarize these columns is by counting the unique string values in the column. We can do this using the `DataFrame.value_counts()` function.\n",
    "\n",
    "Let's see what the different genres in our dataframe are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##EX: Print the most frequent reviewers and artists in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groupby\n",
    "\n",
    "One of the most frequent things we might want to do with a dataframe is to leverage the different groups in our data and obtain summary statistics for them. For example, if we had a dataframe with individuals and their wages, we might want to compare the average wage for men versus women.\n",
    "\n",
    "Pandas has made this easy with the `DataFrame.groupby()` function. You specify the column you want to group your data into. As an example, let's group our data into the different genres, and calculate the average score by genre. We'll do this in a couple of steps.\n",
    "\n",
    "First, create a groupby object by specifying the name of the column in the paranthases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a groupby dataframe grouped by genre\n",
    "df_genres = df.groupby(\"genre\")\n",
    "\n",
    "#What kind of object is df_genres? Let's find out.\n",
    "df_genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a pandas object. We can perform most of the in-built Pandas functions on this object, but we'll see slightly different output than what we saw with a dataframe object.\n",
    "\n",
    "Let's find the average score by genre by calling the DataFrame.mean() function on our grouped by object. We'll discuss the different options we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the mean score by genre, print out the results\n",
    "df_genres['score'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is this output different that the previous time we used the `DataFrame.mean()` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##EX: Print the maximum score for each genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Bonus EX: Find the artist with the highest average score. Find the artist with the lowest average score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating the DTM: scikit-learn\n",
    "\n",
    "Ok, that's the summary of the metadata. Next, we turn to analyzing the text of the reviews. Remember, the text is stored in the 'body' column.\n",
    "\n",
    "Our first step is to turn the text into a document term matrix (DTM). A DTM is a different way of representing text called vector representation. The goal is to turn the text into numbers. To do so, we transform each document into a vector, where each number in the vector represents the count of a particular word. To get a feel for this we'll jump right into an example.\n",
    "\n",
    "We've learned how to count each word using Python's NLTK. We could, then, construct a DTM manually. Luckily, however, Python has a great package, `scikit-learn` with a built-in function to do this called `CountVectorizer()`.\n",
    "\n",
    "[Let's first look at the documentation for CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "We'll implement this by creating a `CountVectorizer()` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the function CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvec = CountVectorizer()\n",
    "\n",
    "#fit and transform our text into a DTM. Ask me about what this code does...\n",
    "sklearn_dtm = countvec.fit_transform(df.body)\n",
    "print(sklearn_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is called Compressed Sparse Format. How do we know what each number indicates? We can access the words themselves through the CountVectorizer function `get_feature_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(countvec.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##EX: What word is indicated by the first row of the DTM printed above?\n",
    "###Hint: Think back to the tutorial on lists, and how to slice a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It save a lot of memory to store the dtm in this format, but it is difficult to look at for a human. To illustrate the techniques in this lesson we will first convert this matrix back to a Pandas dataframe, a format we're more familiar with. For larger datasets, you will have to use the Compressed Sparse Format. Putting it into a DataFrame, however, will enable us to get more comfortable with Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we do the same as we did above, but covert it into a Pandas dataframe\n",
    "#Don't worry about understanding every line of this code\n",
    "dtm_df = pandas.DataFrame(countvec.fit_transform(df.body).toarray(), columns=countvec.get_feature_names(), index = df.index)\n",
    "\n",
    "#view the dtm dataframe\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What can we do with a DTM?\n",
    "\n",
    "We can do a number of calculations using a DTM. For a toy example, we can quickly identify the most frequent words (compare this to how many steps it took in lesson 1, where we found the most frequent words using NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Ex: print the average number of times each word is used in a review\n",
    "#Print this out sorted from highest to lowest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see further stuff we can do with a DTM in days to come. Because it is in the format of a matrix, we can perform any matrix algebra or vector manipulation on it, which enables some pretty exciting things (think vector space and Euclidean  geometry). But, what do we lose when we reprsent text in this format?\n",
    "\n",
    "Today, we will use variations on the DTM to find distinctive words in this dataset.\n",
    "\n",
    "### 4. Tf-idf scores\n",
    "\n",
    "How to find distinctive words in a corpus is a long-standing question in text analysis. We saw two approaches to doing this in lesson 1 (removing stop words and identifying nouns, verbs, and adjectives). Today, we'll learn one more approach: word scores. The idea behind words scores is to weight words not just by their frequency, but by their frequency in one document compared to their distribution across all documents. Words that are frequent, but are also used in every single document, will not be distinguising. We want to identify words that are unevenly distributed across the corpus.\n",
    "\n",
    "One of the most popular ways to weight words (beyond frequency counts) is *tf-idf* scores. By offsetting the frequency of a word by its document frequency (the number of documents in which it appears) will in theory filter out common terms such as 'the', 'of', and 'and'.\n",
    "\n",
    "More precisely, the inverse document frequency is calculated as such:\n",
    "\n",
    "number_of_documents / number_documents_with_term\n",
    "\n",
    "so:\n",
    "\n",
    "tf_idf_word1 = word1_frequency_document1 * (number_of_documents / number_document_with_word1)\n",
    "\n",
    "We can calculate this manually, but scikit-learn has a built-in function to do so. We'll use it, but a challenge for you: use Pandas to calculate this manually. \n",
    "\n",
    "To do so, we simply do the same thing we did above with CountVectorizer, but instead we use the function TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import the function\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define out tfidfvec object\n",
    "tfidfvec = TfidfVectorizer()\n",
    "#create the dtm, but with cells weigthed by the tf-idf score.\n",
    "dtm_tfidf_df = pandas.DataFrame(tfidfvec.fit_transform(df.body).toarray(), columns=tfidfvec.get_feature_names(), index = df.index)\n",
    "\n",
    "#view results\n",
    "dtm_tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still mostly zeros. Let's look at the 20 words with highest tf-idf weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_tfidf_df.max().sort_values(ascending=False)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! We have successfully identified content words, without removing stop words. What else do you notice about this list?\n",
    "\n",
    "### 5. Identifying Distinctive Words\n",
    "\n",
    "What can we do with this? These scores are best used when you want to identify distinctive words for individual documents, or groups of documents, compared to other groups or the corpus as a whole. To illustrate this, let's compare three genres and identify the most distinctive words by genre.\n",
    "\n",
    "First we add the genre of the document into our dtm weighted by tf-idf scores.\n",
    "\n",
    "Why can we do this? Remember dataframes are immutable. What does this mean, and why does this ensure the below code works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creat dataset with document index and genre\n",
    "\n",
    "#firs make a copy of our dtm_tfidf_df\n",
    "dtm_tfidf_df_genre = dtm_tfidf_df\n",
    "\n",
    "#add a GENRE column to it. Why am I making the name of the column GENRE and no genre?\n",
    "dtm_tfidf_df_genre['GENRE'] = df['genre']\n",
    "dtm_tfidf_df_genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare the words with the highest tf-idf weight for each genre. \n",
    "\n",
    "First, create a groupby object, grouped by GENRE, and then create a dataframe out of that object by taking the max of each column. Intuitively, the words with the highest values in each genre are most distinctive to that genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_dtm_genre = dtm_tfidf_df_genre.groupby('GENRE').max()\n",
    "groupby_dtm_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sort the values in the Indie column.\n",
    "groupby_dtm_genre.loc['Indie'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##EX: do the same for Rap and Jazz genres. Compare the most distinctive words. What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! A method of identifying distinctive words. \n",
    "\n",
    "Questions:\n",
    "* How would you describe in words what these lists are telling us about our data?\n",
    "* What theory of language does this measure rely on?\n",
    "* What are the drawbacks of the method for finding distinctive words?\n",
    "* What are other methods we could use to find distinctive words?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
