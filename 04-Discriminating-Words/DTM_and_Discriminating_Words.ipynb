{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Document Term Matrix and Discriminating Words\n",
    "\n",
    "The Document Term Matrix (DTM) in the bread and butter of most computational text analysis techniques, both simple and more sophisticated methods. In this lesson we will use Python's scikit-learn package learn to make a document term matrix from the .csv Music Reviews dataset. We will then use the DTM and a word weighting technique called tf-idf (term frequency inverse document frequency) to identify important and discriminating words within this dataset (utilizing the Pandas package). The illustrating question: what words distinguish reviews of Rap albums, Indie Rock albums, and Jazz albums? Theoretical exerxise: What can we learn from these words?\n",
    "\n",
    "Note: Python's scikit-learn package is an enormous package with a lot of functionality. Knowing this package will enable you to do some very sophisticated analyses, including almost all machine learning techniques. (It looks great on your CV too!). We'll get back to this package later in the workshop.\n",
    "\n",
    "### Learning Goals\n",
    "* Understand the DTM and why it's important to text analysis\n",
    "* Learn how to create a DTM from a .csv file\n",
    "* Learn basic functionality of Python's package scikit-learn (we'll return to scikit-learn in lesson 06)\n",
    "* Understand tf-idf scores, and word scores in general\n",
    "* Learn a simple way to identify distinctive words\n",
    "* In the process, gain more familiarity and comfort with the Pandas pacakge and manipulating data\n",
    "\n",
    "### Outline\n",
    "* The Pandas Dataframe: Music Reviews\n",
    "* Explore the Data using Pandas\n",
    "    * Basic descriptive statistics\n",
    "* Creating the DTM: scikit-learn\n",
    "    * CountVectorizer function\n",
    "* Tf-idf scores\n",
    "    * TfidfVectorizer\n",
    "*  Identifying Distinctive Words\n",
    "    * Identify distinctive reviews by genre\n",
    "\n",
    "\n",
    "### Key Jargon\n",
    "* *Document Term Matrix*:\n",
    "    * a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms.\n",
    "* *TF-IDF Scores*: \n",
    "    *  short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "    \n",
    "### Further Resources\n",
    "\n",
    "[This blog post](https://de.dariah.eu/tatom/feature_selection.html) goes through finding distinctive words using Python in more detail \n",
    "\n",
    "Paper: [Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf), Burt Monroe, Michael Colaresi, Kevin Quinn\n",
    "    \n",
    "### 0. The Pandas Dataframe: Music Reviews\n",
    "\n",
    "First, we read our music reviews corpus, which is stored as a .csv file on our hard drive, into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    album  \\\n",
      "0                             Don't Panic   \n",
      "1                 Fear and Saturday Night   \n",
      "2                      The Way I'm Livin'   \n",
      "3                                   Doris   \n",
      "4                                 Giraffe   \n",
      "5                            Weathervanes   \n",
      "6                    Build a Rocket Boys!   \n",
      "7                      Ambivalence Avenue   \n",
      "8                                 Wavvves   \n",
      "9                          Peachtree Road   \n",
      "10                               Heritage   \n",
      "11                            White Chalk   \n",
      "12                    Tyrannosaurus Hives   \n",
      "13                             JackInABox   \n",
      "14                            Liquid Love   \n",
      "15                  The  Truth About Love   \n",
      "16                            The Monitor   \n",
      "17                         Ones and Sixes   \n",
      "18        In Search Of... [First Version]   \n",
      "19                            Tarot Sport   \n",
      "20                             July Flame   \n",
      "21                                    Lux   \n",
      "22                    Live At The Olympia   \n",
      "23                     Ten Thousand Fists   \n",
      "24                             New Danger   \n",
      "25                                    NYC   \n",
      "26                 Hold On Now, Youngster   \n",
      "27                       Tiny Rebels [EP]   \n",
      "28                       Wavering Radiant   \n",
      "29                          Sound Mirrors   \n",
      "...                                   ...   \n",
      "4971           This Machine Kills Artists   \n",
      "4972                       The Fire Theft   \n",
      "4973           XI Versions of Black Noise   \n",
      "4974              I Dreamed We Fell Apart   \n",
      "4975                         Beat Pyramid   \n",
      "4976                       Freedom's Road   \n",
      "4977          Scott Pilgrim Vs. The World   \n",
      "4978                Little Neon Limelight   \n",
      "4979                                 Hawk   \n",
      "4980                       Single Mothers   \n",
      "4981                        Lost Horizons   \n",
      "4982               Dry Land is Not a Myth   \n",
      "4983                          Pain Killer   \n",
      "4984                    Swing Lo Magellan   \n",
      "4985                             Maraqopa   \n",
      "4986                                  Son   \n",
      "4987                              Holiday   \n",
      "4988                              Majenta   \n",
      "4989                         Love Is Here   \n",
      "4990                                 Days   \n",
      "4991                              Rebirth   \n",
      "4992                           Blue Songs   \n",
      "4993                England, Half English   \n",
      "4994                   Weezer (Red Album)   \n",
      "4995                Dreams and Nightmares   \n",
      "4996                          Outer South   \n",
      "4997                         On An Island   \n",
      "4998                             Movement   \n",
      "4999                          Locked Down   \n",
      "5000  And Their Refinement Of The Decline   \n",
      "\n",
      "                                       artist       genre  \\\n",
      "0                                All Time Low    Pop/Rock   \n",
      "1                                Ryan Bingham     Country   \n",
      "2                              Lee Ann Womack     Country   \n",
      "3                             Earl Sweatshirt         Rap   \n",
      "4                                     Echoboy        Rock   \n",
      "5                            Freelance Whales       Indie   \n",
      "6                                       Elbow    Pop/Rock   \n",
      "7                                       Bibio       Indie   \n",
      "8                                      Wavves       Indie   \n",
      "9                                  Elton John        Rock   \n",
      "10                                    College  Electronic   \n",
      "11                                  PJ Harvey        Rock   \n",
      "12                                  The Hives        Rock   \n",
      "13                               Turin Brakes       Indie   \n",
      "14                                  Shy Child       Indie   \n",
      "15                                       P!nk         Pop   \n",
      "16                           Titus Andronicus       Indie   \n",
      "17                                        Low    Pop/Rock   \n",
      "18                    N.E.R.D. [The Neptunes]         Rap   \n",
      "19                               Fuck Buttons        Rock   \n",
      "20                                Laura Veirs       Indie   \n",
      "21                                  Brian Eno  Electronic   \n",
      "22                                     R.E.M.        Rock   \n",
      "23                                  Disturbed        Rock   \n",
      "24                                    Mos Def         Rap   \n",
      "25               Kieran Hebden and Steve Reid       Indie   \n",
      "26                            Los Campesinos!       Indie   \n",
      "27                                 Cairo Gang    Pop/Rock   \n",
      "28                                       Isis        Rock   \n",
      "29                                    Coldcut  Electronic   \n",
      "...                                       ...         ...   \n",
      "4971                               King Buzzo    Pop/Rock   \n",
      "4972                           The Fire Theft       Indie   \n",
      "4973                         Pantha du Prince  Electronic   \n",
      "4974                                  Memphis       Indie   \n",
      "4975                       These New Puritans        Rock   \n",
      "4976                          John Mellencamp        Rock   \n",
      "4977                      Original Soundtrack        Rock   \n",
      "4978                               Houndmouth     Country   \n",
      "4979                          Isobel Campbell    Pop/Rock   \n",
      "4980                      Justin Townes Earle     Country   \n",
      "4981                              Lemon Jelly  Electronic   \n",
      "4982                             White Arrows    Pop/Rock   \n",
      "4983                          Little Big Town     Country   \n",
      "4984                         Dirty Projectors    Pop/Rock   \n",
      "4985                            Damien Jurado    Pop/Rock   \n",
      "4986                             Juana Molina       Indie   \n",
      "4987                          Port St. Willow    Pop/Rock   \n",
      "4988                              Jimmy Edgar  Electronic   \n",
      "4989                               Starsailor        Rock   \n",
      "4990                              Real Estate    Pop/Rock   \n",
      "4991                                Lil Wayne         Rap   \n",
      "4992                   Hercules & Love Affair  Electronic   \n",
      "4993                              Billy Bragg        Rock   \n",
      "4994                                   Weezer        Rock   \n",
      "4995                                Meek Mill         Rap   \n",
      "4996  Conor Oberst And The Mystic Valley Band       Indie   \n",
      "4997                            David Gilmour        Rock   \n",
      "4998                                   Gossip       Indie   \n",
      "4999                                 Dr. John    Pop/Rock   \n",
      "5000                         Stars Of The Lid        Rock   \n",
      "\n",
      "             release_date                                      critic  score  \\\n",
      "0     2012-10-09 00:00:00                                    Kerrang!   74.0   \n",
      "1     2015-01-20 00:00:00                                       Uncut   70.0   \n",
      "2     2014-09-23 00:00:00                                  Q Magazine   84.0   \n",
      "3     2013-08-20 00:00:00                                   Pitchfork   82.0   \n",
      "4     2003-02-25 00:00:00                                    AllMusic   71.0   \n",
      "5     2010-04-13 00:00:00                                  Q Magazine   68.0   \n",
      "6     2011-04-12 00:00:00                       Delusions of Adequacy   82.0   \n",
      "7     2009-06-23 00:00:00                                  Q Magazine   78.0   \n",
      "8     2009-03-17 00:00:00                                  PopMatters   68.0   \n",
      "9     2004-11-09 00:00:00                                       MelD.   70.0   \n",
      "10    2013-09-17 00:00:00                                musicOMH.com   63.0   \n",
      "11    2007-09-25 00:00:00                              Paste Magazine   80.0   \n",
      "12    2004-07-20 00:00:00                                  Playlouder   78.0   \n",
      "13    2005-06-07 00:00:00                   New Musical Express (NME)   62.0   \n",
      "14    2010-03-15 00:00:00                                The Guardian   70.0   \n",
      "15    2012-09-18 00:00:00                                The Guardian   77.0   \n",
      "16    2010-03-09 00:00:00                             Prefix Magazine   82.0   \n",
      "17    2015-09-11 00:00:00                        Consequence of Sound   78.0   \n",
      "18    2001-08-06 00:00:00                                  Q Magazine   92.0   \n",
      "19    2009-10-20 00:00:00                               The A.V. Club   84.0   \n",
      "20    2010-01-12 00:00:00                                musicOMH.com   81.0   \n",
      "21    2012-11-13 00:00:00                              Paste Magazine   75.0   \n",
      "22    2009-10-27 00:00:00                               The A.V. Club   74.0   \n",
      "23    2005-09-20 00:00:00                                  Amazon.com   59.0   \n",
      "24    2004-10-12 00:00:00                            Austin Chronicle   59.0   \n",
      "25    2008-11-18 00:00:00                                    The Wire   61.0   \n",
      "26    2008-04-01 00:00:00                            Austin Chronicle   81.0   \n",
      "27    2013-07-23 00:00:00                                       Uncut   64.0   \n",
      "28    2009-05-05 00:00:00                                  No Ripcord   79.0   \n",
      "29    2006-02-21 00:00:00                                  Playlouder   73.0   \n",
      "...                   ...                                         ...    ...   \n",
      "4971  2014-06-03 00:00:00                        The Line of Best Fit   69.0   \n",
      "4972  2003-09-23 00:00:00                           Alternative Press   63.0   \n",
      "4973  2011-04-19 00:00:00                          The Boston Phoenix   62.0   \n",
      "4974  2004-09-21 00:00:00                                  Launch.com   67.0   \n",
      "4975  2008-03-18 00:00:00                                  Q Magazine   76.0   \n",
      "4976  2007-01-23 00:00:00                                     Blender   65.0   \n",
      "4977  2010-08-10 00:00:00                             ImmersedInSound   68.0   \n",
      "4978  2015-03-17 00:00:00                                    AllMusic   74.0   \n",
      "4979  2010-08-24 00:00:00                                   Pitchfork   75.0   \n",
      "4980  2014-09-09 00:00:00                               The A.V. Club   73.0   \n",
      "4981  2002-10-08 00:00:00                                       Uncut   81.0   \n",
      "4982  2012-06-19 00:00:00                                    AllMusic   70.0   \n",
      "4983  2014-10-21 00:00:00                                   joyel1992   83.0   \n",
      "4984  2012-07-10 00:00:00                                  PopMatters   80.0   \n",
      "4985  2012-02-20 00:00:00  Beats Per Minute (formerly One Thirty BPM)   81.0   \n",
      "4986  2006-06-06 00:00:00                                    The Wire   79.0   \n",
      "4987  2013-04-02 00:00:00                                musicOMH.com   80.0   \n",
      "4988  2012-12-04 00:00:00                                   Pitchfork   59.0   \n",
      "4989  2002-01-08 00:00:00                                  Screenager   72.0   \n",
      "4990  2011-10-18 00:00:00                              Blurt Magazine   77.0   \n",
      "4991  2010-02-02 00:00:00                                  No Ripcord   37.0   \n",
      "4992  2011-08-16 00:00:00                                  PopMatters   68.0   \n",
      "4993  2002-03-05 00:00:00                                        Spin   64.0   \n",
      "4994  2008-06-03 00:00:00                              Slant Magazine   64.0   \n",
      "4995  2012-10-30 00:00:00                                    AllMusic   69.0   \n",
      "4996  2009-05-05 00:00:00                              Slant Magazine   67.0   \n",
      "4997  2006-03-07 00:00:00                                   E! Online   67.0   \n",
      "4998  2003-05-06 00:00:00                                       Uncut   81.0   \n",
      "4999  2012-04-03 00:00:00                                  PopMatters   86.0   \n",
      "5000  2007-04-07 00:00:00                                  PopMatters   87.0   \n",
      "\n",
      "                                                   body  \n",
      "0     While For Baltimore proves they can still writ...  \n",
      "1     There's nothing fake about the purgatorial nar...  \n",
      "2     All life's disastrous lows are here on a caree...  \n",
      "3     With Doris, Odd Future’s Odysseus is finally b...  \n",
      "4     Though Giraffe is definitely Echoboy's most im...  \n",
      "5     Fans of Owl City and The Postal Service will r...  \n",
      "6     Whereas previous Elbow records set a mood, Bui...  \n",
      "7     His remarkable Warp debut follows a series of ...  \n",
      "8     There’s an energy coursing through this, and r...  \n",
      "9     Classic. Songs filled with soul. Lyrics refres...  \n",
      "10    It’s by no means perfect and it does feel slig...  \n",
      "11    Put in context, White Chalk serves her purpose...  \n",
      "12    Although pretty catchy, this album is a tad to...  \n",
      "13     Talk about a fall from grace. [4 Jun 2005, p.58]  \n",
      "14    It's unusual to find a band equally at home wi...  \n",
      "15    It's just a shame she gave album space to Mari...  \n",
      "16    The fundamental difference between The Monitor...  \n",
      "17    It just needs to be a passionate, cathartic, c...  \n",
      "18    They retained their best ideas for themselves ...  \n",
      "19    For most of the songs amassed here, it still t...  \n",
      "20    Laura Veirs makes an excellent case for hersel...  \n",
      "21    Sure, it's beautiful on its own, but without a...  \n",
      "22    Though it isn’t a concept album, Live At The O...  \n",
      "23    The album isn't without its problems––come the...  \n",
      "24    The New Danger is as overextended as it is sel...  \n",
      "25    Hebden is clearly striving for dancefloor impa...  \n",
      "26    It might be one big, saccharine, catchy fuck-y...  \n",
      "27    The Cairo Gang's superb, weighty, meticulous a...  \n",
      "28    As snobbish as that may sound, you have to los...  \n",
      "29    Any number of tracks here could easily catapul...  \n",
      "...                                                 ...  \n",
      "4971  It is an interesting experiment, but realistic...  \n",
      "4972  Yes, it sounds like Yes, and, no, I don't mean...  \n",
      "4973  The pointlessness is grating. XI Versions' fin...  \n",
      "4974  The basic ingredients are delicate, minimal, w...  \n",
      "4975  Anyone bored by the kitchen sink will find muc...  \n",
      "4976  There are two vastly different Mellencamps. On...  \n",
      "4977  The movie is incredible. Everything about it i...  \n",
      "4978  Houndmouth have the right touch and impressive...  \n",
      "4979  Hawk is very much Campbell's album. She made a...  \n",
      "4980  Earle’s consistency is both his friend and his...  \n",
      "4981  Think early Air meets hip-hop, the West Coast ...  \n",
      "4982  A certain kind of playfulness reigns throughou...  \n",
      "4983  One of the best country album Ive heard in a w...  \n",
      "4984  What really shines through the most on Magella...  \n",
      "4985  These songs are probably Jurado's most ambitio...  \n",
      "4986  What's distinctive about Son is that it is mor...  \n",
      "4987  At times this recording is compelling, entranc...  \n",
      "4988  Unfortunately, more than mediocre tracks or th...  \n",
      "4989  A beautifully crafted debut album and one of t...  \n",
      "4990  A fine batch of bittersweet pop songs that are...  \n",
      "4991             The derivativeness quickly overwhelms.  \n",
      "4992  Blue Songs finds Butler and his crew of collab...  \n",
      "4993  England's exoticism is offset by plenty of tou...  \n",
      "4994  Weezer seems to have driven their old shtick i...  \n",
      "4995  As far as graduations from mixtapes to major-l...  \n",
      "4996  The result is an album that's unfortunately ba...  \n",
      "4997  In the end, Island makes Dave sound like he's ...  \n",
      "4998  Beth Ditto's remarkable gospel holler and ferv...  \n",
      "4999  Dr. John is Dr. John. He's a star, and is on f...  \n",
      "5000  Their work, especially that displayed on Refin...  \n",
      "\n",
      "[5001 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "#create a dataframe called \"df\"\n",
    "df = pandas.read_csv(\"BDHSI2016_music_reviews.csv\", sep = '\\t')\n",
    "\n",
    "#view the dataframe\n",
    "print(df)\n",
    "\n",
    "#notice the metadata. The column \"body\" contains our text of interest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Explore the Data using Pandas\n",
    "\n",
    "Let's first look at some descriptive statistics about this dataset, to get a feel for what's in it. We'll do this using the Pandas package. \n",
    "\n",
    "Note: this is always good practice. It serves two purposes. It checks to make sure your data is correct, and there's no major errors. It also keeps you in touch with your data, which will help with interpretation. <3 your data!\n",
    "\n",
    "First, what genres are in this dataset, and how many reviews in each genre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pop/Rock                  1486\n",
      "Indie                     1115\n",
      "Rock                       932\n",
      "Electronic                 513\n",
      "Rap                        363\n",
      "Pop                        149\n",
      "Country                    140\n",
      "R&B;                       112\n",
      "Folk                        70\n",
      "Alternative/Indie Rock      42\n",
      "Dance                       41\n",
      "Jazz                        38\n",
      "Name: genre, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#We can count this using the value_counts() function\n",
    "print(df['genre'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who are the reviewers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AllMusic                     282\n",
      "PopMatters                   228\n",
      "Pitchfork                    207\n",
      "Q Magazine                   178\n",
      "Uncut                        171\n",
      "Mojo                         137\n",
      "Drowned In Sound             132\n",
      "New Musical Express (NME)    127\n",
      "The A.V. Club                121\n",
      "Rolling Stone                112\n",
      "Under The Radar              100\n",
      "Spin                          97\n",
      "The Guardian                  96\n",
      "musicOMH.com                  88\n",
      "Entertainment Weekly          87\n",
      "Slant Magazine                83\n",
      "Paste Magazine                72\n",
      "Consequence of Sound          69\n",
      "Alternative Press             69\n",
      "Prefix Magazine               68\n",
      "NOW Magazine                  66\n",
      "Tiny Mix Tapes                64\n",
      "Blender                       57\n",
      "Dusted Magazine               56\n",
      "Dot Music                     56\n",
      "Stylus Magazine               55\n",
      "No Ripcord                    53\n",
      "Boston Globe                  52\n",
      "Austin Chronicle              52\n",
      "Filter                        50\n",
      "                            ... \n",
      "petebee                        1\n",
      "AramisG                        1\n",
      "mchlkirk77                     1\n",
      "BONERSMASHER                   1\n",
      "AndrewUltimate                 1\n",
      "KarelV                         1\n",
      "ScarTissue1990                 1\n",
      "audiosonique                   1\n",
      "NikoB                          1\n",
      "clppngfan                      1\n",
      "DiscoStu                       1\n",
      "Elfionnio                      1\n",
      "multe4                         1\n",
      "JimC                           1\n",
      "Jfandango                      1\n",
      "mohamedfayyad                  1\n",
      "JorgeD                         1\n",
      "NOD                            1\n",
      "TunaFish                       1\n",
      "JohnE.                         1\n",
      "NathanCabello                  1\n",
      "EdgarH                         1\n",
      "JamesE                         1\n",
      "BitBurn                        1\n",
      "R                              1\n",
      "joes                           1\n",
      "BloodyPenguin                  1\n",
      "BrianS                         1\n",
      "ElliotZhang                    1\n",
      "PScott                         1\n",
      "Name: critic, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['critic'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the artists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Various Artists            22\n",
      "R.E.M.                     16\n",
      "Arcade Fire                14\n",
      "Sigur Rós                  13\n",
      "Belle & Sebastian          12\n",
      "Brian Eno                  11\n",
      "The Raveonettes            10\n",
      "Weezer                     10\n",
      "Radiohead                  10\n",
      "Low                        10\n",
      "Mogwai                     10\n",
      "Bob Dylan                  10\n",
      "LCD Soundsystem            10\n",
      "Kings of Leon              10\n",
      "Los Campesinos!             9\n",
      "Sun Kil Moon                9\n",
      "Franz Ferdinand             9\n",
      "Wilco                       9\n",
      "Ghostface Killah            9\n",
      "M. Ward                     9\n",
      "Eels                        9\n",
      "Beck                        8\n",
      "Elbow                       8\n",
      "Of Montreal                 8\n",
      "The Decemberists            8\n",
      "Britney Spears              8\n",
      "Daft Punk                   8\n",
      "Usher                       8\n",
      "Kanye West                  8\n",
      "Ryan Adams                  8\n",
      "                           ..\n",
      "Fabolous                    1\n",
      "Eve                         1\n",
      "Small Black                 1\n",
      "PC Worship                  1\n",
      "After the Burial            1\n",
      "Ratking                     1\n",
      "Meg Baird                   1\n",
      "Schneider TM                1\n",
      "Keane                       1\n",
      "Githead                     1\n",
      "The Braids                  1\n",
      "The Vines                   1\n",
      "Tindersticks                1\n",
      "Hank Williams, Jr.          1\n",
      "Keep Shelly in Athens       1\n",
      "Africa Express              1\n",
      "Boston Spaceships           1\n",
      "Magic Trick                 1\n",
      "Hull                        1\n",
      "Texas                       1\n",
      "Miles Davis                 1\n",
      "The Horse's Ha              1\n",
      "Pye Corner Audio            1\n",
      "Jamie xx                    1\n",
      "Toby Keith                  1\n",
      "Numbers                     1\n",
      "Meat Loaf                   1\n",
      "Chairlift                   1\n",
      "Now, Now                    1\n",
      "Rocket Juice & the Moon     1\n",
      "Name: artist, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['artist'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average score given?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.6842231554\n"
     ]
    }
   ],
   "source": [
    "print(df['score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly more complicted to code: what is the average score for each genre? To do this, we use Pandas *groupby* function. Note: If you are planning on doing any sort of statistics, including basic statistics, you'll want to get very familiar with the groupby function. It's quite powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre\n",
      "Jazz                      77.631579\n",
      "Folk                      75.900000\n",
      "Indie                     74.400897\n",
      "Country                   74.071429\n",
      "Alternative/Indie Rock    73.928571\n",
      "Electronic                73.140351\n",
      "Pop/Rock                  73.033782\n",
      "R&B;                      72.366071\n",
      "Rap                       72.173554\n",
      "Rock                      70.754292\n",
      "Dance                     70.146341\n",
      "Pop                       64.608054\n",
      "Name: score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#create a groupby dataframe grouped by genre\n",
    "df_genres = df.groupby(\"genre\")\n",
    "\n",
    "#calculate the mean score by genre, print out the results\n",
    "print(df_genres['score'].mean().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating the DTM: scikit-learn\n",
    "\n",
    "Ok, that's the summary of the metadata. Next, we turn to analyzing the text of the reviews. Remember, the text is stored in the 'body' column.\n",
    "\n",
    "Our first step is to turn the text into a document term matrix using the scikit-learn function called CountVectorizer. There are two ways to do this. We can turn it into a sparse matrix type, which can be used within scikit-learn for further analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16029)\t1\n",
      "  (0, 5755)\t1\n",
      "  (0, 1290)\t1\n",
      "  (0, 11209)\t1\n",
      "  (0, 14577)\t2\n",
      "  (0, 2231)\t1\n",
      "  (0, 13770)\t1\n",
      "  (0, 16272)\t1\n",
      "  (0, 6359)\t1\n",
      "  (0, 1302)\t1\n",
      "  (0, 16017)\t1\n",
      "  (0, 11320)\t1\n",
      "  (0, 14534)\t2\n",
      "  (0, 9218)\t1\n",
      "  (0, 14771)\t2\n",
      "  (0, 7669)\t1\n",
      "  (0, 14818)\t1\n",
      "  (0, 8812)\t1\n",
      "  (0, 13339)\t1\n",
      "  (0, 940)\t1\n",
      "  (0, 3931)\t1\n",
      "  (0, 6688)\t1\n",
      "  (0, 9554)\t1\n",
      "  (0, 15075)\t1\n",
      "  (0, 6629)\t1\n",
      "  :\t:\n",
      "  (5000, 9998)\t1\n",
      "  (5000, 14520)\t1\n",
      "  (5000, 9939)\t1\n",
      "  (5000, 1013)\t1\n",
      "  (5000, 749)\t3\n",
      "  (5000, 16217)\t1\n",
      "  (5000, 14514)\t1\n",
      "  (5000, 1097)\t1\n",
      "  (5000, 14776)\t1\n",
      "  (5000, 16159)\t1\n",
      "  (5000, 9411)\t1\n",
      "  (5000, 13317)\t1\n",
      "  (5000, 9538)\t1\n",
      "  (5000, 1470)\t1\n",
      "  (5000, 14387)\t1\n",
      "  (5000, 5439)\t1\n",
      "  (5000, 13195)\t1\n",
      "  (5000, 10097)\t1\n",
      "  (5000, 11122)\t1\n",
      "  (5000, 13658)\t1\n",
      "  (5000, 11691)\t1\n",
      "  (5000, 4995)\t1\n",
      "  (5000, 12345)\t1\n",
      "  (5000, 5074)\t1\n",
      "  (5000, 4169)\t1\n"
     ]
    }
   ],
   "source": [
    "#import the function CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvec = CountVectorizer()\n",
    "\n",
    "sklearn_dtm = CountVectorizer().fit_transform(df.body)\n",
    "print(sklearn_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This format is called Compressed Sparse Format. It save a lot of memory to store the dtm in this format, but it is difficult to look at for a human. To illustrate the techniques in this lesson we will first convert this matrix back to a Pandas dataframe, a format we're more familiar with. For larger datasets, you will have to use the Compressed Sparse Format. Putting it into a DataFrame, however, will enable us to get more comfortable with Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      00  000  00s  01  03  039  06  08  09  10  ...   zone  zones  zoo  \\\n",
      "0      0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "1      0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "2      0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "3      0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4      0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "5      0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "6      0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "7      0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "8      0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "9      0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "10     0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "11     0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "12     0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "13     0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "14     0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "15     0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "16     0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "17     0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "18     0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "19     0    0    0   0   0    0   0   0   0   1  ...      0      0    0   \n",
      "20     0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "21     0    0    0   0   0    0   0   0   0   0  ...      0      1    0   \n",
      "22     0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "23     0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "24     0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "25     0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "26     0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "27     0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "28     0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "29     0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "...   ..  ...  ...  ..  ..  ...  ..  ..  ..  ..  ...    ...    ...  ...   \n",
      "4971   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4972   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4973   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4974   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4975   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4976   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4977   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4978   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4979   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4980   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4981   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4982   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4983   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4984   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4985   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4986   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4987   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4988   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4989   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4990   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4991   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4992   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4993   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4994   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4995   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4996   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4997   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4998   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "4999   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "5000   0    0    0   0   0    0   0   0   0   0  ...      0      0    0   \n",
      "\n",
      "      zooey  zoomer  zu  zydeco  álbum  être  über  \n",
      "0         0       0   0       0      0     0     0  \n",
      "1         0       0   0       0      0     0     0  \n",
      "2         0       0   0       0      0     0     0  \n",
      "3         0       0   0       0      0     0     0  \n",
      "4         0       0   0       0      0     0     0  \n",
      "5         0       0   0       0      0     0     0  \n",
      "6         0       0   0       0      0     0     0  \n",
      "7         0       0   0       0      0     0     0  \n",
      "8         0       0   0       0      0     0     0  \n",
      "9         0       0   0       0      0     0     0  \n",
      "10        0       0   0       0      0     0     0  \n",
      "11        0       0   0       0      0     0     0  \n",
      "12        0       0   0       0      0     0     0  \n",
      "13        0       0   0       0      0     0     0  \n",
      "14        0       0   0       0      0     0     0  \n",
      "15        0       0   0       0      0     0     0  \n",
      "16        0       0   0       0      0     0     0  \n",
      "17        0       0   0       0      0     0     0  \n",
      "18        0       0   0       0      0     0     0  \n",
      "19        0       0   0       0      0     0     0  \n",
      "20        0       0   0       0      0     0     0  \n",
      "21        0       0   0       0      0     0     0  \n",
      "22        0       0   0       0      0     0     0  \n",
      "23        0       0   0       0      0     0     0  \n",
      "24        0       0   0       0      0     0     0  \n",
      "25        0       0   0       0      0     0     0  \n",
      "26        0       0   0       0      0     0     0  \n",
      "27        0       0   0       0      0     0     0  \n",
      "28        0       0   0       0      0     0     0  \n",
      "29        0       0   0       0      0     0     0  \n",
      "...     ...     ...  ..     ...    ...   ...   ...  \n",
      "4971      0       0   0       0      0     0     0  \n",
      "4972      0       0   0       0      0     0     0  \n",
      "4973      0       0   0       0      0     0     0  \n",
      "4974      0       0   0       0      0     0     0  \n",
      "4975      0       0   0       0      0     0     0  \n",
      "4976      0       0   0       0      0     0     0  \n",
      "4977      0       0   0       0      0     0     0  \n",
      "4978      0       0   0       0      0     0     0  \n",
      "4979      0       0   0       0      0     0     0  \n",
      "4980      0       0   0       0      0     0     0  \n",
      "4981      0       0   0       0      0     0     0  \n",
      "4982      0       0   0       0      0     0     0  \n",
      "4983      0       0   0       0      0     0     0  \n",
      "4984      0       0   0       0      0     0     0  \n",
      "4985      0       0   0       0      0     0     0  \n",
      "4986      0       0   0       0      0     0     0  \n",
      "4987      0       0   0       0      0     0     0  \n",
      "4988      0       0   0       0      0     0     0  \n",
      "4989      0       0   0       0      0     0     0  \n",
      "4990      0       0   0       0      0     0     0  \n",
      "4991      0       0   0       0      0     0     0  \n",
      "4992      0       0   0       0      0     0     0  \n",
      "4993      0       0   0       0      0     0     0  \n",
      "4994      0       0   0       0      0     0     0  \n",
      "4995      0       0   0       0      0     0     0  \n",
      "4996      0       0   0       0      0     0     0  \n",
      "4997      0       0   0       0      0     0     0  \n",
      "4998      0       0   0       0      0     0     0  \n",
      "4999      0       0   0       0      0     0     0  \n",
      "5000      0       0   0       0      0     0     0  \n",
      "\n",
      "[5001 rows x 16415 columns]\n"
     ]
    }
   ],
   "source": [
    "#we do the same as we did above, but covert it into a Pandas dataframe\n",
    "dtm_df = pandas.DataFrame(countvec.fit_transform(df.body).toarray(), columns=countvec.get_feature_names(), index = df.index)\n",
    "\n",
    "#view the dtm dataframe\n",
    "print(dtm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What can we do with a DTM?\n",
    "\n",
    "We can do a number of calculations using a DTM. For a toy example, we can quickly identify the most frequent words (compare this to how many steps it took in lesson 1, where we found the most frequent words using NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the              7406\n",
      "and              4557\n",
      "of               4400\n",
      "to               3175\n",
      "is               2914\n",
      "it               2608\n",
      "that             2039\n",
      "in               1775\n",
      "album            1719\n",
      "this             1518\n",
      "but              1439\n",
      "with             1367\n",
      "as               1310\n",
      "on               1139\n",
      "for              1073\n",
      "are               812\n",
      "their             775\n",
      "you               775\n",
      "an                751\n",
      "his               743\n",
      "more              712\n",
      "be                691\n",
      "like              681\n",
      "from              676\n",
      "not               650\n",
      "songs             640\n",
      "they              580\n",
      "one               580\n",
      "its               575\n",
      "all               574\n",
      "                 ... \n",
      "football            1\n",
      "foothold            1\n",
      "footnote            1\n",
      "footprints          1\n",
      "footsteps           1\n",
      "footwork            1\n",
      "footy               1\n",
      "foppish             1\n",
      "reveling            1\n",
      "revelling           1\n",
      "forbears            1\n",
      "revels              1\n",
      "stefani             1\n",
      "steeple             1\n",
      "steep               1\n",
      "forcefully          1\n",
      "stealthy            1\n",
      "forcible            1\n",
      "forcing             1\n",
      "fore                1\n",
      "stealth             1\n",
      "forefathers         1\n",
      "foreground          1\n",
      "foregrounding       1\n",
      "necessitates        1\n",
      "foreigner           1\n",
      "stealing            1\n",
      "forest              1\n",
      "reverberating       1\n",
      "flights             1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dtm_df.sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see further stuff we can do with a DTM in days to come. Because it is in the format of a matrix, we can perform any matrix algebra or vector manipulation on it, which enables some pretty exciting things (think vector space and Euclidean  geometry). But, what do we lose when we reprsent text in this format?\n",
    "\n",
    "Today, we will use variations on the DTM to find distinctive words in this dataset.\n",
    "\n",
    "### 4. Tf-idf scores\n",
    "\n",
    "How to find distinctive words in a corpus is a long-standing question in text analysis. We saw two approaches to doing this in lesson 1 (removing stop words and identifying nouns, verbs, and adjectives). Today, we'll learn one more approach: word scores. The idea behind words scores is to weight words not just by their frequency, but by their frequency in one document compared to their distribution across all documents. Words that are frequent, but are also used in every single document, will not be distinguising. We want to identify words that are unevenly distributed across the corpus.\n",
    "\n",
    "One of the most popular ways to weight words (beyond frequency counts) is *tf-idf* scores. By offsetting the frequency of a word by its document frequency (the number of documents in which it appears) will in theory filter out common terms such as 'the', 'of', and 'and'.\n",
    "\n",
    "More precisely, the inverse document frequency is calculated as such:\n",
    "\n",
    "number_of_documents / number_documents_with_term\n",
    "\n",
    "so:\n",
    "\n",
    "tf_idf_word1 = word1_frequency_document1 * (number_of_documents / number_document_with_word1)\n",
    "\n",
    "We can calculate this manually, but scikit-learn has a built-in function to do so. We'll use it, but a challenge for you: use Pandas to calculate this manually. \n",
    "\n",
    "To do so, we simply do the same thing we did above with CountVectorizer, but instead we use the function TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       00  000  00s   01   03  039   06   08   09        10  ...   zone  \\\n",
      "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "5     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "6     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "7     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "8     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "9     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "10    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "11    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "12    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "13    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "14    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "15    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "16    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "17    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "18    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "19    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.133194  ...    0.0   \n",
      "20    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "21    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "22    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "23    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "24    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "25    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "26    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "27    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "28    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "29    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...       ...  ...    ...   \n",
      "4971  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4972  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4973  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4974  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4975  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4976  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4977  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4978  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4979  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4980  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4981  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4982  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4983  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4984  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4985  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4986  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4987  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4988  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4989  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4990  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4991  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4992  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4993  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4994  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4995  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4996  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4997  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4998  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "4999  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "5000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.000000  ...    0.0   \n",
      "\n",
      "         zones  zoo  zooey  zoomer   zu  zydeco  álbum  être  über  \n",
      "0     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "1     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "2     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "3     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "5     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "6     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "7     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "8     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "9     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "10    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "11    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "12    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "13    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "14    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "15    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "16    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "17    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "18    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "19    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "20    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "21    0.316818  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "22    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "23    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "24    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "25    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "26    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "27    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "28    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "29    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "...        ...  ...    ...     ...  ...     ...    ...   ...   ...  \n",
      "4971  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4972  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4973  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4974  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4975  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4976  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4977  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4978  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4979  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4980  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4981  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4982  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4983  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4984  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4985  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4986  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4987  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4988  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4989  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4990  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4991  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4992  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4993  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4994  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4995  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4996  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4997  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4998  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4999  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "5000  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "\n",
      "[5001 rows x 16415 columns]\n"
     ]
    }
   ],
   "source": [
    "#import the function\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfvec = TfidfVectorizer()\n",
    "#create the dtm, but with cells weigthed by the tf-idf score.\n",
    "dtm_tfidf_df = pandas.DataFrame(tfidfvec.fit_transform(df.body).toarray(), columns=tfidfvec.get_feature_names(), index = df.index)\n",
    "\n",
    "#view results\n",
    "print(dtm_tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still mostly zeros. Let's look at the 20 words with highest tf-idf weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perfect       1.000000\n",
      "pppperfect    1.000000\n",
      "meh           1.000000\n",
      "awesome       1.000000\n",
      "brill         1.000000\n",
      "yummy         1.000000\n",
      "wonderfull    1.000000\n",
      "subpar        0.959257\n",
      "ga            0.908259\n",
      "masterful     0.898620\n",
      "grower        0.888624\n",
      "likable       0.867803\n",
      "acirc         0.867003\n",
      "great         0.864253\n",
      "infectious    0.859996\n",
      "blank         0.854475\n",
      "smart         0.847852\n",
      "8217          0.843505\n",
      "stuff         0.834479\n",
      "impeccable    0.828662\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(dtm_tfidf_df.max().sort_values(ascending=False)[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! We have successfully identified content words, without removing stop words. What else do you notice about this list?\n",
    "\n",
    "### 5. Identifying Distinctive Words\n",
    "\n",
    "What can we do with this? These scores are best used when you want to identify distinctive words for individual documents, or groups of documents, compared to other groups or the corpus as a whole. To illustrate this, let's compare three genres and identify the most distinctive words by genre.\n",
    "\n",
    "First we merge the genre of the document into our dtm weighted by tf-idf scores, and then compare genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           genre\n",
      "0       Pop/Rock\n",
      "1        Country\n",
      "2        Country\n",
      "3            Rap\n",
      "4           Rock\n",
      "5          Indie\n",
      "6       Pop/Rock\n",
      "7          Indie\n",
      "8          Indie\n",
      "9           Rock\n",
      "10    Electronic\n",
      "11          Rock\n",
      "12          Rock\n",
      "13         Indie\n",
      "14         Indie\n",
      "15           Pop\n",
      "16         Indie\n",
      "17      Pop/Rock\n",
      "18           Rap\n",
      "19          Rock\n",
      "20         Indie\n",
      "21    Electronic\n",
      "22          Rock\n",
      "23          Rock\n",
      "24           Rap\n",
      "25         Indie\n",
      "26         Indie\n",
      "27      Pop/Rock\n",
      "28          Rock\n",
      "29    Electronic\n",
      "...          ...\n",
      "4971    Pop/Rock\n",
      "4972       Indie\n",
      "4973  Electronic\n",
      "4974       Indie\n",
      "4975        Rock\n",
      "4976        Rock\n",
      "4977        Rock\n",
      "4978     Country\n",
      "4979    Pop/Rock\n",
      "4980     Country\n",
      "4981  Electronic\n",
      "4982    Pop/Rock\n",
      "4983     Country\n",
      "4984    Pop/Rock\n",
      "4985    Pop/Rock\n",
      "4986       Indie\n",
      "4987    Pop/Rock\n",
      "4988  Electronic\n",
      "4989        Rock\n",
      "4990    Pop/Rock\n",
      "4991         Rap\n",
      "4992  Electronic\n",
      "4993        Rock\n",
      "4994        Rock\n",
      "4995         Rap\n",
      "4996       Indie\n",
      "4997        Rock\n",
      "4998       Indie\n",
      "4999    Pop/Rock\n",
      "5000        Rock\n",
      "\n",
      "[5001 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#creat dataset with document index and genre\n",
    "df_genre = df['genre'].to_frame()\n",
    "print(df_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         genre_x   00  000  00s   01   03  039   06   08   09  ...   zone  \\\n",
      "0       Pop/Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "1        Country  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "2        Country  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "3            Rap  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4           Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "5          Indie  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "6       Pop/Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "7          Indie  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "8          Indie  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "9           Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "10    Electronic  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "11          Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "12          Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "13         Indie  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "14         Indie  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "15           Pop  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "16         Indie  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "17      Pop/Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "18           Rap  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "19          Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "20         Indie  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "21    Electronic  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "22          Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "23          Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "24           Rap  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "25         Indie  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "26         Indie  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "27      Pop/Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "28          Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "29    Electronic  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "...          ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...   \n",
      "4971    Pop/Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4972       Indie  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4973  Electronic  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4974       Indie  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4975        Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4976        Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4977        Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4978     Country  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4979    Pop/Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4980     Country  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4981  Electronic  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4982    Pop/Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4983     Country  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4984    Pop/Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4985    Pop/Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4986       Indie  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4987    Pop/Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4988  Electronic  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4989        Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4990    Pop/Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4991         Rap  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4992  Electronic  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4993        Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4994        Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4995         Rap  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4996       Indie  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4997        Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4998       Indie  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "4999    Pop/Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "5000        Rock  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0   \n",
      "\n",
      "         zones  zoo  zooey  zoomer   zu  zydeco  álbum  être  über  \n",
      "0     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "1     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "2     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "3     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "5     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "6     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "7     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "8     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "9     0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "10    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "11    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "12    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "13    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "14    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "15    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "16    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "17    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "18    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "19    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "20    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "21    0.316818  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "22    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "23    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "24    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "25    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "26    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "27    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "28    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "29    0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "...        ...  ...    ...     ...  ...     ...    ...   ...   ...  \n",
      "4971  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4972  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4973  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4974  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4975  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4976  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4977  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4978  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4979  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4980  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4981  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4982  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4983  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4984  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4985  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4986  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4987  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4988  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4989  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4990  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4991  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4992  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4993  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4994  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4995  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4996  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4997  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4998  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "4999  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "5000  0.000000  0.0    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
      "\n",
      "[5001 rows x 16416 columns]\n"
     ]
    }
   ],
   "source": [
    "#merge this into the dtm_tfidf_df\n",
    "merged_df = df_genre.join(dtm_tfidf_df, how = 'right', lsuffix='_x')\n",
    "\n",
    "#view result\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare the words with the highest tf-idf weight for each genre. \n",
    "\n",
    "Note: there are other ways to do this. Challenge: what is a different approach to identifying rows from a certain genre in our dtm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rap Words\n",
      "blank             0.854475\n",
      "039               0.797595\n",
      "waste             0.755918\n",
      "amiable           0.730963\n",
      "awesomely         0.717079\n",
      "same              0.672391\n",
      "sucker            0.663760\n",
      "tight             0.653993\n",
      "beastie           0.650603\n",
      "lamest            0.639377\n",
      "derivativeness    0.636271\n",
      "authentic         0.627192\n",
      "diverse           0.623373\n",
      "sermon            0.621175\n",
      "mastermind        0.609213\n",
      "neat              0.608922\n",
      "we                0.600755\n",
      "lift              0.591821\n",
      "supreme           0.590431\n",
      "overwhelms        0.586293\n",
      "dtype: float64\n",
      "\n",
      "Indie Words\n",
      "underplayed    0.516717\n",
      "prisoner       0.512087\n",
      "jezabels       0.512087\n",
      "careworn       0.509386\n",
      "folk           0.476719\n",
      "victory        0.449289\n",
      "exhausted      0.445969\n",
      "bigger         0.441849\n",
      "heyday         0.438114\n",
      "babelfished    0.431543\n",
      "bet            0.426091\n",
      "worn           0.416482\n",
      "93             0.416137\n",
      "try            0.415525\n",
      "triumph        0.413976\n",
      "silhouette     0.413374\n",
      "icelandic      0.411715\n",
      "fourth         0.409213\n",
      "nonetheless    0.406747\n",
      "riffed         0.402439\n",
      "dtype: float64\n",
      "\n",
      "Jazz Words\n",
      "devotional      0.507724\n",
      "clarinet        0.470680\n",
      "amidst          0.465227\n",
      "duhnam          0.456224\n",
      "carper          0.456224\n",
      "purely          0.455232\n",
      "descending      0.441013\n",
      "apply           0.435261\n",
      "68              0.433787\n",
      "holds           0.425910\n",
      "recordings      0.418013\n",
      "mose            0.414479\n",
      "allison         0.414479\n",
      "languid         0.414254\n",
      "spacier         0.412838\n",
      "sidelined       0.412838\n",
      "playful         0.409639\n",
      "determinedly    0.402711\n",
      "innovative      0.400028\n",
      "dynamic         0.399175\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#pull out the reviews for three genres, Rap, Alternative/Indie Rock, and Jazz\n",
    "dtm_rap = merged_df[merged_df['genre_x']==\"Rap\"]\n",
    "dtm_indie = merged_df[merged_df['genre_x']==\"Alternative/Indie Rock\"]\n",
    "dtm_jazz = merged_df[merged_df['genre_x']==\"Jazz\"]\n",
    "\n",
    "#print the words with the highest tf-idf scores for each genre\n",
    "print(\"Rap Words\")\n",
    "print(dtm_rap.max(numeric_only=True).sort_values(ascending=False)[0:20])\n",
    "print()\n",
    "print(\"Indie Words\")\n",
    "print(dtm_indie.max(numeric_only=True).sort_values(ascending=False)[0:20])\n",
    "print()\n",
    "print(\"Jazz Words\")\n",
    "print(dtm_jazz.max(numeric_only=True).sort_values(ascending=False)[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! A method of identifying distinctive words. You notice there are some proper nouns in there. How might we remove those if we're not interested in them?\n",
    "\n",
    "Tf-idf scores are just one way to identify distinctive or discriminating words. See Monroe, Colaresi, and Quinn (2009) for more ideas for finding distinctive words. (Warning: this paper is a bit outdated. No one has taken up their recommendation to use a Dirichlet prior).\n",
    "\n",
    "Exercise: \n",
    "* Compare words from different genres of your choice. Any interesting findings\n",
    "* Instead of outputting the highest weighted words, output the lowest weighted words. How should we interpret these words? \n",
    "* Super challenge: apply this technique to your own corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
