{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "# <center> #DHBSI 2016: Computational Text Analysis </center>\n",
    "\n",
    "## <center> Laura Nelson <br/> <em>Postdoctoral Fellow | Digital Humanities @ Berkeley | Berkeley Institute for Data Science </em> </center>\n",
    "\n",
    "## <center> Teddy Roland <br/> <em> Coordinator, Digital Humanities @ Berkeley <br/> Lecturer, UC Berkeley </em> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Summary </center>\n",
    "## <center> Text Analysis Demystified </center>\n",
    "### <center> It's Just Counting! <br/> </center>\n",
    "![Counting](Text_Counting.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> The Dark Side of DH: An Invitation\n",
    "![Dark Side](Dark_Side.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Text Analysis in Research </center>\n",
    "![Interpretive Moments](Text_Analysis_In_Reearch.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Lessons </center>\n",
    "### <center> Our workshop included 5 days and 7 lessons to learn how counting, sometimes creative counting, can amplify and augment close readings of text </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1: Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "punctuations = list(string.punctuation)\n",
    "\n",
    "#read the two text files from your hard drive, assign first mystery text to variable 'text1' and second mystery text to variable 'text2'\n",
    "text1 = open('../01-Intro-to-NLP/text1.txt').read()\n",
    "text2 = open('../01-Intro-to-NLP/text2.txt').read()\n",
    "\n",
    "###word frequencies\n",
    "\n",
    "#tokenize texts\n",
    "text1_tokens = word_tokenize(text1)\n",
    "text2_tokens = word_tokenize(text2)\n",
    "\n",
    "#pre-process for word frequency\n",
    "#lowercase\n",
    "text1_tokens_lc = [word.lower() for word in text1_tokens]\n",
    "text2_tokens_lc = [word.lower() for word in text2_tokens]\n",
    "\n",
    "#remove stopwords\n",
    "text1_tokens_clean = [word for word in text1_tokens_lc if word not in stopwords.words('english')]\n",
    "text2_tokens_clean = [word for word in text2_tokens_lc if word not in stopwords.words('english')]\n",
    "\n",
    "#remove punctuation using the list of punctuation from the string pacage\n",
    "text1_tokens_clean = [word for word in text1_tokens_clean if word not in punctuations]\n",
    "text2_tokens_clean = [word for word in text2_tokens_clean if word not in punctuations]\n",
    "\n",
    "#frequency distribution\n",
    "text1_word_frequency = nltk.FreqDist(text1_tokens_clean)\n",
    "text2_word_frequency = nltk.FreqDist(text2_tokens_clean)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Frequent Words for Text1\")\n",
    "print(\"________________________\")\n",
    "for word in text1_word_frequency.most_common(20):\n",
    "    print(word[0])\n",
    "print()\n",
    "print(\"Frequent Words for Text2\")\n",
    "print(\"________________________\")\n",
    "for word in text2_word_frequency.most_common(20):\n",
    "    print(word[0])\n",
    "    \n",
    "    \n",
    "### Can you guess the novel from most frequent words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Basics of Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nothing to see here, folks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Operationalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "dialogue_df = pandas.read_csv('../03-Operationalizing/antigone_dialogue.csv', index_col=0)\n",
    "dialogue_tokens = [character.split() for character in dialogue_df['DIALOGUE']]\n",
    "dialogue_len = [len(tokens) for tokens in dialogue_tokens]\n",
    "dialogue_df['WORDS_SPOKEN'] = dialogue_len\n",
    "dialogue_df = dialogue_df.sort_values('WORDS_SPOKEN', ascending = False)\n",
    "# Let's visualize!\n",
    "\n",
    "# Tells Jupyter to produce images in notebook\n",
    "% pylab inline\n",
    "\n",
    "# Makes images look good\n",
    "style.use('ggplot')\n",
    "dialogue_df['WORDS_SPOKEN'].plot(kind='bar')\n",
    "\n",
    "###Who is the main protagonist? Maybe not Antigone?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Discriminating Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = pandas.read_csv(\"../04-Discriminating-Words/BDHSI2016_music_reviews.csv\", sep = '\\t')\n",
    "\n",
    "tfidfvec = TfidfVectorizer()\n",
    "#create the dtm, but with cells weigthed by the tf-idf score.\n",
    "dtm_tfidf_df = pandas.DataFrame(tfidfvec.fit_transform(df.body).toarray(), columns=tfidfvec.get_feature_names(), index = df.index)\n",
    "\n",
    "df_genre = df['genre'].to_frame()\n",
    "merged_df = df_genre.join(dtm_tfidf_df, how = 'right', lsuffix='_x')\n",
    "\n",
    "#pull out the reviews for three genres, Rap, Alternative/Indie Rock, and Jazz\n",
    "dtm_rap = merged_df[merged_df['genre_x']==\"Rap\"]\n",
    "dtm_indie = merged_df[merged_df['genre_x']==\"Alternative/Indie Rock\"]\n",
    "dtm_jazz = merged_df[merged_df['genre_x']==\"Jazz\"]\n",
    "\n",
    "#print the words with the highest tf-idf scores for each genre\n",
    "print(\"Rap Words\")\n",
    "print(dtm_rap.max(numeric_only=True).sort_values(ascending=False)[0:20])\n",
    "print()\n",
    "print(\"Indie Words\")\n",
    "print(dtm_indie.max(numeric_only=True).sort_values(ascending=False)[0:20])\n",
    "print()\n",
    "print(\"Jazz Words\")\n",
    "print(dtm_jazz.max(numeric_only=True).sort_values(ascending=False)[0:20])\n",
    "\n",
    "###What words are distinct to reviews of Rap albums, Indie albums, and Jazz albums?\n",
    "##Notice the word weights for the Rap albums compared to others. Are these reviews more different than other reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Sentiment Analysis using the Dictionary Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_sent = open(\"../05-Dictionary-Method/positive_words.txt\").read()\n",
    "neg_sent = open(\"../05-Dictionary-Method/negative_words.txt\").read()\n",
    "\n",
    "positive_words=pos_sent.split('\\n')\n",
    "negative_words=neg_sent.split('\\n')\n",
    "\n",
    "text1_pos = [word for word in text1_tokens_clean if word in positive_words]\n",
    "text2_pos = [word for word in text2_tokens_clean if word in positive_words]\n",
    "\n",
    "text1_neg = [word for word in text1_tokens if word in negative_words]\n",
    "text2_neg = [word for word in text2_tokens if word in negative_words]\n",
    "\n",
    "print(\"Postive words in Melville\")\n",
    "print(len(text1_pos)/len(text1_tokens))\n",
    "print()\n",
    "print(\"Negative words in Melville\")\n",
    "print(len(text1_neg)/len(text1_tokens))\n",
    "print()\n",
    "print(\"Postive words in Austen\")\n",
    "print(len(text2_pos)/len(text2_tokens))\n",
    "print()\n",
    "print(\"Negative words in Austen\")\n",
    "print(len(text2_neg)/len(text2_tokens))\n",
    "\n",
    "## Who is more postive, Melville or Austen?\n",
    "## Melville has a similar precentage of postive and negative words (a whale is a whale, neither good nor bad)\n",
    "## Austen is decidedly more positive than negative (it's the gentleman thing to do)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6: Literary Distinction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "\n",
    "review_path = '../06-Literary Distinction (Probably)/poems/reviewed/'\n",
    "random_path = '../06-Literary Distinction (Probably)/poems/random/'\n",
    "review_files = os.listdir(review_path)\n",
    "random_files = os.listdir(random_path)\n",
    "review_texts = [open(review_path+file_name).read() for file_name in review_files]\n",
    "random_texts = [open(random_path+file_name).read() for file_name in random_files]\n",
    "all_texts = review_texts + random_texts\n",
    "all_file_names = review_files + random_files\n",
    "all_labels = ['reviewed'] * len(review_texts) + ['random'] * len(random_texts)\n",
    "cv = CountVectorizer(stop_words = 'english', min_df=180, binary = True, max_features = None)\n",
    "dtm = cv.fit_transform(all_texts).toarray()\n",
    "nb = MultinomialNB()\n",
    "nb.fit(dtm, all_labels)\n",
    "\n",
    "dickinson_canonic = \"\"\"Because I could not stop for Death – \n",
    "He kindly stopped for me –  \n",
    "The Carriage held but just Ourselves –  \n",
    "And Immortality.\n",
    "\n",
    "We slowly drove – He knew no haste\n",
    "And I had put away\n",
    "My labor and my leisure too,\n",
    "For His Civility – \n",
    "\n",
    "We passed the School, where Children strove\n",
    "At Recess – in the Ring –  \n",
    "We passed the Fields of Gazing Grain –  \n",
    "We passed the Setting Sun – \n",
    "\n",
    "Or rather – He passed us – \n",
    "The Dews drew quivering and chill – \n",
    "For only Gossamer, my Gown – \n",
    "My Tippet – only Tulle – \n",
    "\n",
    "We paused before a House that seemed\n",
    "A Swelling of the Ground – \n",
    "The Roof was scarcely visible – \n",
    "The Cornice – in the Ground – \n",
    "\n",
    "Since then – ‘tis Centuries – and yet\n",
    "Feels shorter than the Day\n",
    "I first surmised the Horses’ Heads \n",
    "Were toward Eternity – \"\"\"\n",
    "\n",
    "\n",
    "anthem_patriotic = \"\"\"O! say can you see, by the dawn's early light,\n",
    "What so proudly we hailed at the twilight's last gleaming,\n",
    "Whose broad stripes and bright stars through the perilous fight,\n",
    "O'er the ramparts we watched, were so gallantly streaming?\n",
    "And the rockets' red glare, the bombs bursting in air,\n",
    "Gave proof through the night that our flag was still there;\n",
    "O! say does that star-spangled banner yet wave\n",
    "O'er the land of the free and the home of the brave?\"\"\"\n",
    "\n",
    "unknown_dtm = cv.transform([dickinson_canonic,anthem_patriotic]).toarray()\n",
    "nb.predict(unknown_dtm)\n",
    "\n",
    "## Can a computer predict whether a poem would be considered 'presitgious'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6: Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas\n",
    "from nltk.corpus import stopwords, words\n",
    "\n",
    "metadata_df = pandas.read_csv('../07-Topic Modeling/txtlab_Novel150_English.csv')\n",
    "fiction_path = '../07-Topic Modeling/txtalb_Novel150_English/'\n",
    "novel_list = [open(fiction_path+file_name).read() for file_name in metadata_df['filename']]\n",
    "novel_tokens_list = [novel.lower().split() for novel in novel_list]\n",
    "dictionary = gensim.corpora.dictionary.Dictionary(novel_tokens_list)\n",
    "proper_names = [word.lower() for word in words.words() if word.istitle()]\n",
    "noise_tokens = [word for word in dictionary.values() if word.isalpha()==False or len(word)<=2]\n",
    "bad_words = stopwords.words('english') + proper_names + noise_tokens\n",
    "stop_ids = [_id for _id, count in dictionary.doc2bow(bad_words)]\n",
    "dictionary.filter_tokens(bad_ids = stop_ids)\n",
    "dictionary.filter_extremes(no_below = 40)\n",
    "corpus = [dictionary.doc2bow(text) for text in novel_tokens_list]\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=25, alpha='auto', id2word=dictionary, iterations=2500, passes = 4)\n",
    "list_of_doctopics = [lda_model.get_document_topics(text, minimum_probability=0) for text in corpus]\n",
    "list_of_probabilities = [[probability for label,probability in distribution] for distribution in list_of_doctopics]\n",
    "proba_distro_df = pandas.DataFrame(list_of_probabilities)\n",
    "metadata_df = pandas.concat([metadata_df, pandas.DataFrame(list_of_probabilities)], axis=1)\n",
    "annual_means_df = metadata_df.groupby('date').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "annual_means_df[8].plot(kind='bar', figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_model.show_topic(8)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
