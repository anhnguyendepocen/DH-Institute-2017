{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>It Starts with a Research Question...</h1>\n",
    "<img src='Nelson, Mining the Dispatch, NYTimes Opinionator.jpg' width=\"66%\" height=\"66%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling in Python\n",
    "<ul><li>Review/Preview</li>\n",
    "<ul><li>String Methods</li>\n",
    "<li>List Comprehensions</li>\n",
    "<li>Concatenation</li>\n",
    "<li>Group DataFrame by Rows</li>\n",
    "</ul>\n",
    "<li>Pre-Process</li>\n",
    "<ul><li>Import Corpus</li>\n",
    "<li>Tokenize</li>\n",
    "<li>Feature Selection</li>\n",
    "</ul>\n",
    "<li>Topic Model</li>\n",
    "<li>Interpreting the Model</li>\n",
    "<ul><li>Visualization</li></ul>\n",
    "<li>Revising Model Inputs</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Corpus Description\n",
    "English-language subset of Andrew Piper's novel corpus, totaling 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n",
    "\n",
    "The image at the top of this notebook comes from Robert K. Nelson's project <i>Mining the Distpatch</i>. The study uses topic modeling to expore the <i>Richmond Dispatch</i>, the Confederacy's paper of record during the American Civil War. It demonstrates the interpretive power of laying out topic distributions over a chronological axis.\n",
    "\n",
    "### Metadata Columns\n",
    "<ol><li>Filename: Name of file on disk</li>\n",
    "<li>ID: Unique ID in Piper corpus</li>\n",
    "<li>Language: Language of novel</li>\n",
    "<li>Date: Initial publication date</li>\n",
    "<li>Title: Title of novel</li>\n",
    "<li>Gender: Authorial gender</li>\n",
    "<li>Person: Textual perspective</li>\n",
    "<li>Length: Number of tokens in novel</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Review/Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "style.use('ggplot')\n",
    "\n",
    "import pandas\n",
    "import nltk\n",
    "\n",
    "modules = [\"words\", \"stopwords\"]\n",
    "nltk.download(modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign a string to a variable\n",
    "\n",
    "a_token = 'Spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make it lower case\n",
    "\n",
    "a_token.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test whether the original word was lower case\n",
    "\n",
    "a_token.islower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test whether the original word is title case\n",
    "\n",
    "a_token.istitle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Is it alphabetical?\n",
    "\n",
    "a_token.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New token with punctuation\n",
    "\n",
    "excited_token = 'Spam!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Still counts as title case?\n",
    "\n",
    "excited_token.istitle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Still counts as alphabetical?\n",
    "\n",
    "excited_token.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How long is it?\n",
    "\n",
    "len(excited_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Longer than that first token, right?\n",
    "\n",
    "len(excited_token) > len(a_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Comprehensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's use a longer string\n",
    "\n",
    "script = \"Man: Well, what've you got? Waitress: Well, there's egg and bacon; egg sausage and bacon; \\\n",
    "egg and spam; egg bacon and spam; egg bacon sausage and spam; spam bacon sausage and spam; \\\n",
    "spam egg spam spam bacon and spam; spam sausage spam spam bacon spam tomato and spam; \\\n",
    "spam spam spam egg and spam; spam spam spam spam spam spam baked beans spam spam spam; \\\n",
    "...or Lobster Thermidor au Crevette with a Mornay sauce served in a Provencale manner with shallots \\\n",
    "and aubergines garnished with truffle pate, brandy and with a fried egg on top and spam.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split it into tokens\n",
    "\n",
    "script.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign to a variable\n",
    "\n",
    "script_tokens = script.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the title case tokens\n",
    "\n",
    "[token for token in script_tokens if token.istitle()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the NOT title case tokens\n",
    "\n",
    "[token for token in script_tokens if not token.istitle()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Multiple conditions!\n",
    "\n",
    "[token for token in script_tokens if token.isalpha() and not token.istitle()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List in which each entry has two elements\n",
    "\n",
    "paired_entries = [['yes','no'],['yes','no'],['yes','no']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "paired_entries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic list comprehension format\n",
    "\n",
    "[pair for pair in paired_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Call up entry items individually\n",
    "\n",
    "[first for first, second in paired_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[second for first, second in paired_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. For the list below, return the first number in each pair.\n",
    "\n",
    "## EX. For the list below, return the first number in each pair if it is greater than the second number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_pairs = [ [1,12], [15,6], [18,17], [3,9], [21,16] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Two lists in which each element is also a list\n",
    "\n",
    "list_1 = [['bacon','eggs','sausage'],['tomato','beans','lobster']]\n",
    "list_2 = [['spam','spam','spam'],['spam','spam','shallots']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A list of lists\n",
    "\n",
    "list_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Concatenate the lists\n",
    "\n",
    "list_1 + list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a dataframe with the concatenated lists\n",
    "# Number of rows matches the length of the concatenated list\n",
    "\n",
    "pandas.DataFrame(list_1 + list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert each list to a dataframe individually\n",
    "\n",
    "df_1 = pandas.DataFrame(list_1)\n",
    "df_2 = pandas.DataFrame(list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It's a dataframe!\n",
    "\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Concatenate the lists as columns rather than rows\n",
    "\n",
    "pandas.concat([df_1,df_2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group DataFrame Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a new dataframe\n",
    "\n",
    "columns = ['Menu Item','With Spam?', 'Price']\n",
    "values = [['Lobster','Yes',12],\n",
    "          ['Eggs','Yes',6],\n",
    "          ['Beans','No',5],\n",
    "          ['Bacon','No',2],\n",
    "          ['Bacon','Yes',3]]\n",
    "menu_df = pandas.DataFrame(values, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It's a menu!\n",
    "\n",
    "menu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get basic statistics for numeric columns\n",
    "\n",
    "menu_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# But what if I want to divvy up those stats based on whether there is spam in the dish?\n",
    "\n",
    "menu_df.groupby('With Spam?').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What if I just want one of the stats\n",
    "# Count is handy to see how many items are in each category\n",
    "\n",
    "menu_df.groupby('With Spam?').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just one column of interest\n",
    "\n",
    "menu_df.groupby('With Spam?').count()['Menu Item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Of course, the average is very handy\n",
    "\n",
    "menu_df.groupby('With Spam?').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And why not graph it\n",
    "\n",
    "menu_df.groupby('With Spam?').mean().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Import the corpus metadata in the cell below. Get a list of the number of books\n",
    "##     published in each year.\n",
    "\n",
    "## EX. Find the average text length by year. Graph this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata_df = pandas.read_csv('txtlab_Novel150_English.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-Process\n",
    "\n",
    "Typically, this is the process of importing a corpus and then converting it into a Document-Term Matrix. However, gensim (our Topic Modeling package) prefers to receive texts as token lists. It then converts the vocabulary of the corpus into a <i>dictionary</i> (not to be confused with the Python datatype) that maps words to unique ID's.\n",
    "\n",
    "We perform feature selection by subtracting words from that dictionary. Topic Modeling is especially sensitive to stopwords, proper names, and errors introduced by digitization, so we will make a point of removing those tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read metadata\n",
    "\n",
    "metadata_df = pandas.read_csv('txtlab_Novel150_English.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set location of corpus folder\n",
    "\n",
    "fiction_path = 'txtalb_Novel150_English/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Corpus\n",
    "\n",
    "novel_list = [open(fiction_path+file_name).read() for file_name in metadata_df['filename']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "novel_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split each novel into a list of tokens\n",
    "\n",
    "novel_tokens_list = [novel.lower().split() for novel in novel_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect tokens from first novel\n",
    "\n",
    "novel_tokens_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection (Gensim Dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Topic Model package\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dictionary based on corpus tokens\n",
    "# Each token is mapped to its own unique ID\n",
    "\n",
    "dictionary = gensim.corpora.dictionary.Dictionary(novel_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Map lists of tokens to the dictionary IDs\n",
    "\n",
    "dictionary.doc2bow(['pride','prejudice', 'pride'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove stopwords & (some!) proper names from dictionary\n",
    "\n",
    "from nltk.corpus import stopwords, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our trusty list of stop words\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of common English-language words, typically used for autocorrect\n",
    "\n",
    "words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Proper name test\n",
    "\n",
    "'Ishmael' in words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find proper names by looking for title-case words, then make lower case\n",
    "\n",
    "proper_names = [word.lower() for word in words.words() if word.istitle()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The list of all words in the dictionary\n",
    "\n",
    "list(dictionary.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noise_tokens = [word for word in dictionary.values() if word.isalpha()==False or len(word)<=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Collect stop words and proper names together\n",
    "\n",
    "bad_words = stopwords.words('english') + proper_names + noise_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Rather than passing a list of stopwords to gensim, we pass in their dictionary ids\n",
    "\n",
    "dictionary.doc2bow(bad_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map stopwords, proper names to dictionary IDs\n",
    "\n",
    "stop_ids = [_id for _id, count in dictionary.doc2bow(bad_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "stop_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove stopwords from dictionary mappings\n",
    "\n",
    "dictionary.filter_tokens(bad_ids = stop_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove terms by document frequency -- in this case about a quarter of all documents\n",
    "\n",
    "dictionary.filter_extremes(no_below = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of dictionary mappings by novel\n",
    "# This is gensim's version of a document-term matrix\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in novel_tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect first text's representation\n",
    "\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA) Models\n",
    "LDA reflects an intuition that words in a text are not merely chosen at random but are drawn from underlying concepts (the so-called \"latent variables\"). The goal of LDA is to look across many texts in order to reverse engineer these concepts by finding words that tend to cluster with one another. For this reason, LDA has been referred to as \"the mother of all word collocation techniques.\"\n",
    "\n",
    "### Topic Model Features\n",
    "<ul><li>Corpus: Pre-processed textual corpus</li>\n",
    "<li>Number of Topics: Choosing this is the art of Topic Modeling </li>\n",
    "<li>Alpha (Hyperparameter): Prior, reflecting expected distribution of topics over documents</li>\n",
    "<li>Iterations: TM initially uses random distribution, iteratively tweaks model</li>\n",
    "<li>Passes: Bootstrap method for evaluating model during training; primarily seen in Gensim implementation</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train Topic Model\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=25, alpha='auto', \\\n",
    "                                   id2word=dictionary, iterations=2500, passes = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you have more than two cores at your disposal, then perhaps try:\n",
    "\n",
    "#lda_model = gensim.models.ldamulticore.LdaMulticore(corpus, num_topics=25, \\\n",
    "#                                                    id2word=dictionary, iterations=2500, passes = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quick look at n topics among those inferred\n",
    "\n",
    "lda_model.show_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Deeper look at particular topic\n",
    "\n",
    "lda_model.show_topic(8, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Return a list of the top 20 words for topic 0.\n",
    "##     Return a list of all words for topic 0.\n",
    "\n",
    "## EX. Using the 'show_topics' method, try to find a topic in your model that is similar to \n",
    "##     one in that of person sitting next to you. How closely related do the topics seem?\n",
    "\n",
    "## CHALLENGE: Create a table that contains all topic-term distributions.\n",
    "##            Make each row a certain topic and label each column by the word it represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Most prominent topics in a given document\n",
    "\n",
    "lda_model.get_document_topics(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribution of all topics over a document\n",
    "\n",
    "lda_model.get_document_topics(corpus[0], minimum_probability=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Return a list of the most prominent topics in document 10.\n",
    "##     What terms are most prominent in those topics?\n",
    "\n",
    "## EX. Compare your answers to the previous exercise with a classmate.\n",
    "##     Do similar topics come up? Different ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Measure of model's \"fit\" to corpus data\n",
    "# Related to the probability of seeing texts like the ones in our corpus given inferred model\n",
    "\n",
    "lda_model.log_perplexity(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Most present topics in corpus\n",
    "\n",
    "lda_model.top_topics(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Interpeting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "There are many strategies that can be used to interpret the output of a topic model. In this case, we will visualize topics over time in order to look for patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of all document-topic distributions\n",
    "\n",
    "list_of_doctopics = [lda_model.get_document_topics(text, minimum_probability=0) for text in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect\n",
    "\n",
    "list_of_doctopics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In the list above, each topic got represented as a tuple containing\n",
    "# the label of the topic and its probability within the given document\n",
    "\n",
    "# Create list containing only the probabilities (remains ordered by topic label)\n",
    "list_of_probabilities = [[probability for label,probability in distribution] for distribution in list_of_doctopics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Labels removed!\n",
    "\n",
    "list_of_probabilities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reformat as a DataFrame\n",
    "# Each row is a given text; each column is the probability distribution of a topic\n",
    "\n",
    "pandas.DataFrame(list_of_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign to variable\n",
    "\n",
    "proba_distro_df = pandas.DataFrame(list_of_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Concatenate our dataframe of metadata with the new one of document-topic distributions\n",
    "\n",
    "pandas.concat([metadata_df, proba_distro_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reassign concanated dataframe to 'metadata_df'\n",
    "\n",
    "metadata_df = pandas.concat([metadata_df, pandas.DataFrame(list_of_probabilities)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group the rows of our dataframe by the date of each book's publication\n",
    "# Get the average of each numberical value listed for that year\n",
    "\n",
    "metadata_df.groupby('date').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assign this to a new variable so we can play around with it easily\n",
    "\n",
    "annual_means_df = metadata_df.groupby('date').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect mean topic distribution by year\n",
    "\n",
    "annual_means_df[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot mean topic distribution by year\n",
    "\n",
    "annual_means_df[8].plot(kind='bar', figsize=(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# And let's glance back at the most prominent terms in that topic\n",
    "\n",
    "lda_model.show_topic(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Alternate Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Q.  Some proper names and titles still came through our filter.\n",
    "##     How might you remove names in a more targeted way?\n",
    "\n",
    "## EX. For Matt Jockers's study of literary theme in 'Macroanalysis',\n",
    "##     he included only nouns for topic modeling. Use a POS tagger\n",
    "##     to remove all words from the corpus that are not common nouns.\n",
    "\n",
    "## EX. Jockers also found it useful to split texts into 1000-noun chunks\n",
    "##     after the POS filter. Run the topic model over these smaller chunks.\n",
    "##     Do the topics appear different?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
