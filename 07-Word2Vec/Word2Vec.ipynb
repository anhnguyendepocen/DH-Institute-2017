{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'>It Starts with a Research Question...</h1>\n",
    "<img src='Schmidt Fig 1.png' width=\"66%\" height=\"66%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "This lesson is designed to explore features of word embeddings produced through the word2vec model. The questions we ask in this lesson are guided by Ben Schmidt's blog post <a href = \"http://bookworm.benschmidt.org/posts/2015-10-30-rejecting-the-gender-binary.html\">Rejecting the Gender Binary</a>.\n",
    "\n",
    "The primary corpus we wil use consists of the <a href=\"http://txtlab.org/?p=601\">150 English-language novels</a> made available by the <em>.txtLab</em> at McGill University. We also look at a <a href=\"http://ryanheuser.org/word-vectors-1/\">Word2Vec model trained on the ECCO-TCP corpus</a> of 2,350 eighteenth-century literary texts made available by Ryan Heuser. (Note that the number of terms in the model has been shortened by half in order to conserve memory.)\n",
    "\n",
    "For further background on Word2Vec's mechanics, I suggest this <a href=\"https://www.tensorflow.org/versions/r0.8/tutorials/word2vec/index.html\">brief tutorial</a> by Google, especially the sections \"Motivation,\" \"Skip-Gram Model,\" and \"Visualizing.\"\n",
    "\n",
    "### Workshop Agenda\n",
    "<ul>\n",
    "<li>Vector-Space Model of Language</li>\n",
    "<li>Import & Pre-Processing</li>\n",
    "<li>Word2Vec</li>\n",
    "<ul><li>Training</li>\n",
    "<li>Embeddings</li>\n",
    "<li>Visualization</li>\n",
    "</ul>\n",
    "<li>Saving/Loading Models</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Vector-Space Model of Language\n",
    "In the first day of this workshop, we explored the strange way that computers read text: by splitting it into tokens and tallying their frequencies. A novel or album review is reduced to a series of word counts. Since then, we have used simple arithmetic and statistics to identify patterns across those tallies. However, it is often useful to consider these patterns from another perspective: geometry.\n",
    "\n",
    "Each text can be described as a series of word counts. What if we treated those like coordinates in space?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DTM with a Few Pseudo-Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes!\n",
    "import pandas\n",
    "\n",
    "# Construct dataframe\n",
    "columns = ['eggs','sausage','bacon']\n",
    "indices = ['Novel A', 'Novel B', 'Novel C']\n",
    "dtm = [[50,60,60],[90,10,10], [20,70,70]]\n",
    "dtm_df = pandas.DataFrame(dtm, columns = columns, index = indices)\n",
    "\n",
    "# Show dataframe\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot our points\n",
    "scatter(dtm_df['eggs'], dtm_df['sausage'])\n",
    "\n",
    "# Make the graph look good\n",
    "xlim([0,100]), ylim([0,100])\n",
    "xlabel('eggs'), ylabel('sausage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors\n",
    "\n",
    "At a glance, a couple of points are lying closer to one another. We used the word frequencies of just two words in order to plot our texts in a two-dimensional plane. The term frequency \"summaries\" of <i>Novel A</i> & <i>Novel C</i> are pretty similar to one another: they both share a major concern with \"eggs\", whereas <i>Novel B</i> seems to focus primarily on \"sausage.\"\n",
    "\n",
    "This raises a question: how can we operationalize our intuition that spatial distance expresses topical similarity?\n",
    "\n",
    "<img src='Dot-Product.png' >\n",
    "\n",
    "The most common measurement of distance between points is their <a href=\"https://en.wikipedia.org/wiki/Cosine_similarity\">Cosine Similarity</a>. Imagine that we draw an arrow from the origin of the graph -- point (0,0) -- to the dot representing each text. This arrow is called a <i>vector</i>. The Cosine Similarity between two vectors measures how much they overlap with one another. Values for the cosine similarity between two vectors fall between 0 and 1, so they are easily interpreted and compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Although we want the Cosine Distance, it is mathematically\n",
    "# simpler to calculate its opposite: Cosine Similarity\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we will subtract the similarities from 1\n",
    "\n",
    "cos_sim = cosine_similarity(dtm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we'll make it a little easier to read\n",
    "\n",
    "np.round(cos_sim, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Semantics\n",
    "\n",
    "The above method has a broad range of applications, such as unsupervised clustering. Common techniques include <a href = \"https://en.wikipedia.org/wiki/K-means_clustering\">K-Means Clustering</a> and <a href=\"https://en.wikipedia.org/wiki/Hierarchical_clustering\">Heirchical Dendrograms</a>. These attempt to identify groups of texts with shared content, based on these kinds of distance measures.\n",
    "\n",
    "For this lesson, however, we will turn this logic on its head. Rather than produce vectors representing texts based on their words, we will produce vectors for the words based on their contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn our DTM sideways\n",
    "\n",
    "dtm_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the Cosine Distances between pairs of word-vectors\n",
    "\n",
    "cos_sim = cosine_similarity(dtm_df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In readable format\n",
    "\n",
    "np.round(cos_sim, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "This last cell indicates that \"sausage\" and \"bacon\" perfectly align with one another across texts. If we saw this in a real-world example, we might interpret it to mean that the words share some kind of semantic or thematic relationship. In fact, this method is precisely one that humanists have used in order to find interesting linguistic patterns. (See Ted Underwood's blog post, <a href = \"https://tedunderwood.com/2011/10/16/lsa-is-a-marvellous-tool-but-humanists-may-no-use-it-the-way-computer-scientists-do/\">LSA is a marvellous tool, but...</a>.)\n",
    "\n",
    "Recently, however, a more sophisticated technique for finding semantic relationships between words has enjoyed great popularity: Word2Vec\n",
    "\n",
    "Word2Vec draws from the logic of the concordance that we saw on the first day of the workshop. In the example above, a word (row) is described by its frequency within an entire novel (column). The word2vec algorithm tries to describe a given word in terms of the ones that appear immediately to the right and left in actual sentences. More precisely it learns how to <i>predict</i> the context words.\n",
    "\n",
    "<img src = \"Skip-gram diagram.png\">\n",
    "\n",
    "Without going too deeply into the algorithm, suffice it to say that it involves a two-step process. First, the input word gets compressed into a dense vector. Second, the vector gets decoded into the set of context words. Keywords that appear within similar contexts will have similar vector representations in between steps. This vector is referred to as a <i>word embedding</i>.\n",
    "\n",
    "Since the word embedding is a vector, we can perform tests like cosine similarity and other kinds of operations. Those operations can reveal many different kinds of relationships between words, as we shall see.\n",
    "\n",
    "<img src = \"w2v-Analogies.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import & Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Wrangling\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.manifold import MDS, TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Custom Tokenizer for Classroom Use\n",
    "\n",
    "def fast_tokenize(text):\n",
    "    \n",
    "    # Get a list of punctuation marks\n",
    "    from string import punctuation\n",
    "    \n",
    "    lower_case = text.lower()\n",
    "    \n",
    "    # Iterate through text removing punctuation characters\n",
    "    no_punct = \"\".join([char for char in lower_case if char not in punctuation])\n",
    "    \n",
    "    # Split text over whitespace into list of words\n",
    "    tokens = no_punct.split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Description\n",
    "English-language subset of Andrew Piper's novel corpus, totaling 150 novels by British and American authors spanning the years 1771-1930. These texts reside on disk, each in a separate plaintext file. Metadata is contained in a spreadsheet distributed with the novel files.\n",
    "\n",
    "### Metadata Columns\n",
    "<ol><li>Filename: Name of file on disk</li>\n",
    "<li>ID: Unique ID in Piper corpus</li>\n",
    "<li>Language: Language of novel</li>\n",
    "<li>Date: Initial publication date</li>\n",
    "<li>Title: Title of novel</li>\n",
    "<li>Gender: Authorial gender</li>\n",
    "<li>Person: Textual perspective</li>\n",
    "<li>Length: Number of tokens in novel</li></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Metadata into Pandas Dataframe\n",
    "\n",
    "meta_df = pandas.read_csv('resources/txtlab_Novel450_English.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check Metadata\n",
    "\n",
    "meta_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set location of corpus folder\n",
    "\n",
    "fiction_folder = 'txtlab_Novel450_English/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collect the text of each file in the 'fiction_folder' on the hard drive\n",
    "\n",
    "# Create empty list, each entry will be the string for a given novel\n",
    "novel_list = []\n",
    "\n",
    "# Iterate through filenames in 'fiction_folder'\n",
    "for filename in os.listdir(fiction_folder):\n",
    "    \n",
    "    # Read novel text as single string\n",
    "    with open(fiction_folder + filename, 'r') as file_in:\n",
    "        this_novel = file_in.read()\n",
    "    \n",
    "    # Add novel text as single string to master list\n",
    "    novel_list.append(this_novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect first item in novel_list\n",
    "\n",
    "novel_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "Word2Vec learns about the relationships among words by observing them in context. This means that we want to split our texts into word-units. However, we  want to maintain sentence boundaries as well, since the last word of the previous sentence might skew the meaning of the next sentence.\n",
    "\n",
    "Since novels were imported as single strings, we'll first use <i>sent_tokenize</i> to divide them into sentences, and second, we'll split each sentence into its own list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split each novel into sentences\n",
    "\n",
    "sentences = [sentence for novel in novel_list for sentence in sent_tokenize(novel)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect first sentence\n",
    "\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split each sentence into tokens\n",
    "\n",
    "words_by_sentence = [fast_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove any sentences that contain zero tokens\n",
    "\n",
    "words_by_sentence = [sentence for sentence in words_by_sentence if sentence != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect first sentence\n",
    "\n",
    "words_by_sentence[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Word Embedding\n",
    "Word2Vec is the most prominent word embedding algorithm. Word embedding generally attempts to identify semantic relationships between words by observing them in context.\n",
    "\n",
    "Imagine that each word in a novel has its meaning determined by the ones that surround it in a limited window. For example, in Moby Dick's first sentence, “me” is paired on either side by “Call” and “Ishmael.” After observing the windows around every word in the novel (or many novels), the computer will notice a pattern in which “me” falls between similar pairs of words to “her,” “him,” or “them.” Of course, the computer had gone through a similar process over the words “Call” and “Ishmael,” for which “me” is reciprocally part of their contexts.  This chaining of signifiers to one another mirrors some of humanists' most sophisticated interpretative frameworks of language.\n",
    "\n",
    "The two main flavors of Word2Vec are CBOW (Continuous Bag of Words) and Skip-Gram, which can be distinguished partly by their input and output during training. Skip-Gram takes a word of interest as its input (e.g. \"me\") and tries to learn how to predict its context words (\"Call\",\"Ishmael\"). CBOW does the opposite, taking the context words (\"Call\",\"Ishmael\") as a single input and tries to predict the word of interest (\"me\").\n",
    "\n",
    "In general, CBOW is is faster and does well with frequent words, while Skip-Gram potentially represents rare words better.\n",
    "\n",
    "### Word2Vec Features\n",
    "<ul>\n",
    "<li>Size: Number of dimensions for word embedding model</li>\n",
    "<li>Window: Number of context words to observe in each direction</li>\n",
    "<li>min_count: Minimum frequency for words included in model</li>\n",
    "<li>sg (Skip-Gram): '0' indicates CBOW model; '1' indicates Skip-Gram</li>\n",
    "<li>Alpha: Learning rate (initial); prevents model from over-correcting, enables finer tuning</li>\n",
    "<li>Iterations: Number of passes through dataset</li>\n",
    "<li>Batch Size: Number of words to sample from data during each pass</li>\n",
    "</ul>\n",
    "\n",
    "Note: Script uses default value for each argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train word2vec model from txtLab corpus\n",
    "\n",
    "model = gensim.models.Word2Vec(words_by_sentence, size=100, window=5, \\\n",
    "                               min_count=25, sg=1, alpha=0.025, iter=5, batch_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Return dense word vector\n",
    "\n",
    "model['whale']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector-Space Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity\n",
    "Since words are represented as dense vectors, we can ask how similiar words' meanings are based on their cosine similarity (essentially how much they overlap). <em>gensim</em> has a few dout-of-the-box functions that enable different kinds of comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cosine distance between two given word vectors\n",
    "\n",
    "model.similarity('pride','prejudice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find nearest word vectors by cosine distance\n",
    "\n",
    "model.most_similar('pride')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a list of words, we can ask which doesn't belong\n",
    "\n",
    "# Finds mean vector of words in list\n",
    "# and identifies the word further from that mean\n",
    "\n",
    "model.doesnt_match(['pride','prejudice', 'whale'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Valences\n",
    "A word embedding may encode both primary and secondary meanings that are both present at the same time. In order to identify secondary meanings in a word, we can subtract the vectors of primary (or simply unwanted) meanings. For example, we may wish to remove the sense of <em>river bank</em> from the word <em>bank</em>. This would be written mathetmatically as <em>RIVER - BANK</em>, which in <em>gensim</em>'s interface lists <em>RIVER</em> as a positive meaning and <em>BANK</em> as a negative one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most similar words to BANK, in order\n",
    "# to get a sense for its primary meaning\n",
    "\n",
    "model.most_similar('bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the sense of \"river bank\" from \"bank\" and see what is left\n",
    "\n",
    "model.most_similar(positive=['bank'], negative=['river'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogy\n",
    "Analogies are rendered as simple mathematical operations in vector space. For example, the canonic word2vec analogy <em>MAN is to KING as WOMAN is to ??</em> is rendered as <em>KING - MAN + WOMAN</em>. In the gensim interface, we designate <em>KING</em> and <em>WOMAN</em> as positive terms and <em>MAN</em> as a negative term, since it is subtracted from those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most similar words to KING, in order\n",
    "# to get a sense for its primary meaning\n",
    "\n",
    "model.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The canonic word2vec analogy: King - Man + Woman -> Queen\n",
    "\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gendered Vectors\n",
    "Can we find gender a la Schmidt (2015)? (Note that this method uses vector projection, whereas Schmidt had used rejection.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feminine Vector\n",
    "\n",
    "model.most_similar(positive=['she','her','hers','herself'], negative=['he','him','his','himself'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masculine Vector\n",
    "\n",
    "model.most_similar(positive=['he','him','his','himself'], negative=['she','her','hers','herself'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Use the most_similar method to find the tokens nearest to 'car' in our model.\n",
    "##     Do the same for 'motorcar'.\n",
    "\n",
    "## Q.  What characterizes each word in our corpus? Does this make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. How does our model answer the analogy: MADRID is to SPAIN as PARIS is to __________\n",
    "\n",
    "## Q.  What has our model learned about nation-states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Perform the canonic Word2Vec addition again but leave out a term:\n",
    "##     Try 'king' - 'man', 'woman' - 'man', 'woman' + 'king'\n",
    "\n",
    "## Q.  What do these indicate semantically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of words in model\n",
    "\n",
    "model.wv.vocab\n",
    "#model.vocab # deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the whole vocabulary would make it hard to read\n",
    "\n",
    "len(model.wv.vocab)\n",
    "#len(model.vocab) # deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For interpretability, we'll select words that already have a semantic relation\n",
    "\n",
    "her_tokens = [token for token,weight in model.most_similar(positive=['she','her','hers','herself'], \\\n",
    "                                                       negative=['he','him','his','himself'], topn=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inspect list\n",
    "\n",
    "her_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the vector for each sampled word\n",
    "\n",
    "vectors = [model[word] for word in her_tokens]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate distances among texts in vector space\n",
    "\n",
    "dist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multi-Dimensional Scaling (Project vectors into 2-D)\n",
    "\n",
    "mds = MDS(n_components = 2, dissimilarity='precomputed')\n",
    "embeddings = mds.fit_transform(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a pretty graph\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\n",
    "for i in range(len(vectors)):\n",
    "    ax.annotate(her_tokens[i], ((embeddings[i,0], embeddings[i,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison, here is the same graph using a masculine-pronoun vector\n",
    "\n",
    "his_tokens = [token for token,weight in model.most_similar(positive=['he','him','his','himself'], \\\n",
    "                                                       negative=['she','her','hers','herself'], topn=50)]\n",
    "vectors = [model[word] for word in his_tokens]\n",
    "dist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\n",
    "mds = MDS(n_components = 2, dissimilarity='precomputed')\n",
    "embeddings = mds.fit_transform(dist_matrix)\n",
    "_, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\n",
    "for i in range(len(vectors)):\n",
    "    ax.annotate(his_tokens[i], ((embeddings[i,0], embeddings[i,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Q. What kinds of semantic relationships exist in the diagram above?\n",
    "##    Are there any words that seem out of place?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Saving/Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save current model for later use\n",
    "\n",
    "model.wv.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt')\n",
    "#model.save_word2vec_format('resources/word2vec.txtlab_Novel150_English.txt') # deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load up models from disk\n",
    "\n",
    "# Model trained on Eighteenth Century Collections Online corpus (~2500 texts)\n",
    "# Made available by Ryan Heuser: http://ryanheuser.org/word-vectors-1/\n",
    "\n",
    "ecco_model = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.ECCO-TCP.txt')\n",
    "#ecco_model = gensim.models.Word2Vec.load_word2vec_format('resources/word2vec.ECCO-TCP.txt') # deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are similar words to BANK?\n",
    "\n",
    "ecco_model.most_similar('bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we remove the sense of \"river bank\"?\n",
    "\n",
    "ecco_model.most_similar(positive=['bank'], negative=['river'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EX. Heuser's blog post explores an analogy in eighteenth-century thought that\n",
    "##     RICHES are to VIRTUE what LEARNING is to GENIUS. How true is this in\n",
    "##     the ECCO-trained Word2Vec model? Is it true in the one we trained?\n",
    "\n",
    "##  Q. How might we compare word2vec models more generally?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECCO model: RICHES are to VIRTUE what LEARNING is to ??\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# txtLab model: RICHES are to VIRTUE what LEARNING is to ??\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Open Questions\n",
    "At this point, we have seen a number of mathemetical operations that we may use to explore word2vec's word embeddings. These enable us to answer a set of new, interesting questions dealing with semantics, yet there are many other questions that remain unanswered.\n",
    "\n",
    "For example:\n",
    "<ol>\n",
    "<li>How to compare word usages in different texts (within the same model)?</li>\n",
    "<li>How to compare word meanings in different models? compare whole models?</li>\n",
    "<li>What about the space “in between” words?</li>\n",
    "<li>Do we agree with the Distributional Hypothesis that words with the same contexts share their meanings?</li>\n",
    "<ol><li>If not, then what information do we think is encoded in a word’s context?</li></ol>\n",
    "<li>What good, humanistic research questions do analogies shed light on?</li>\n",
    "<ol><li>shades of meaning?</li><li>context similarity?</li></ol>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
